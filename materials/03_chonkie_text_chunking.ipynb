{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Chonkie.ai - Advanced Text Chunking for RAG Applications\n",
        "\n",
        "`by JoÃ£o Gabriel Lima`\n",
        "\n",
        "## ğŸ¯ **Objetivo**\n",
        "\n",
        "Este notebook explora **Chonkie.ai**, uma biblioteca Python especializada em chunking de texto para aplicaÃ§Ãµes RAG (Retrieval-Augmented Generation). Vamos analisar as 9 estratÃ©gias de chunking disponÃ­veis, implementar casos de uso prÃ¡ticos e avaliar performance.\n",
        "\n",
        "## ğŸ“‹ **ConteÃºdo**\n",
        "\n",
        "1. [IntroduÃ§Ã£o e InstalaÃ§Ã£o](#intro)\n",
        "2. [ConfiguraÃ§Ã£o e Imports](#config)\n",
        "3. [AnÃ¡lise Comparativa dos Chunkers](#comparison)\n",
        "4. [ImplementaÃ§Ã£o PrÃ¡tica](#implementation)\n",
        "5. [Casos de Uso EspecÃ­ficos](#use-cases)\n",
        "6. [AnÃ¡lise de Performance](#performance)\n",
        "7. [Troubleshooting e LimitaÃ§Ãµes](#troubleshooting)\n",
        "8. [ConclusÃµes e RecomendaÃ§Ãµes](#conclusions)\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”§ **PrÃ©-requisitos**\n",
        "\n",
        "- Python 3.8+\n",
        "- Conhecimento bÃ¡sico de NLP e RAG\n",
        "- Familiaridade com embeddings e modelos de linguagem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ“¦ **InstalaÃ§Ã£o** <a id=\"intro\"></a>\n",
        "\n",
        "### InstalaÃ§Ã£o BÃ¡sica\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "shell"
        }
      },
      "source": [
        "# InstalaÃ§Ã£o bÃ¡sica\n",
        "uv add chonkie\n",
        "\n",
        "# InstalaÃ§Ã£o completa com todos os recursos\n",
        "uv add \"chonkie[all]\"\n",
        "\n",
        "# DependÃªncias adicionais para testes abrangentes\n",
        "uv add tiktoken sentence-transformers transformers torch numpy pandas matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "uv add chonkie"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## âš™ï¸ **ConfiguraÃ§Ã£o e Imports** <a id=\"config\"></a>\n",
        "\n",
        "### Imports Essenciais\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core Chonkie imports\n",
        "from chonkie import (\n",
        "    TokenChunker,\n",
        "    SentenceChunker, \n",
        "    RecursiveChunker,\n",
        "    SemanticChunker,\n",
        "    SDPMChunker,\n",
        "    LateChunker,\n",
        "    NeuralChunker,\n",
        "    SlumberChunker,\n",
        "    CodeChunker\n",
        ")\n",
        "\n",
        "# Additional utilities\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "from typing import List, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ… Imports realizados com sucesso!\")\n",
        "print(f\"ğŸ“Š Chunkers disponÃ­veis: {len([TokenChunker, SentenceChunker, RecursiveChunker, SemanticChunker, SDPMChunker, LateChunker, NeuralChunker, SlumberChunker, CodeChunker])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n",
        "## ğŸ” **AnÃ¡lise Comparativa dos Chunkers** <a id=\"comparison\"></a>\n",
        "\n",
        "### Chunkers Comparison Table\n",
        "\n",
        "A tabela abaixo detalha as principais diferenÃ§as, casos de uso e funcionamento de cada chunker disponÃ­vel no Chonkie.ai:\n",
        "\n",
        "| Chunker | EstratÃ©gia | Caso de Uso Ideal | Performance | Complexidade | Requer Modelo |\n",
        "|---------|------------|-------------------|-------------|--------------|---------------|\n",
        "| **TokenChunker** | DivisÃ£o por tokens fixos | APIs com limites de tokens, controle preciso | Muito Alta | Baixa | Tokenizer |\n",
        "| **SentenceChunker** | DivisÃ£o por limites de sentenÃ§a | Documentos bem estruturados, textos narrativos | Alta | Baixa | NÃ£o |\n",
        "| **RecursiveChunker** | DivisÃ£o hierÃ¡rquica com regras customizÃ¡veis | Documentos hierÃ¡rquicos, papers acadÃªmicos | MÃ©dia-Alta | MÃ©dia | NÃ£o |\n",
        "| **SemanticChunker** | Agrupamento por similaridade semÃ¢ntica | Documentos longos, conteÃºdo heterogÃªneo | MÃ©dia | Alta | Embedding Model |\n",
        "| **SDPMChunker** | Semantic Double-Pass Merge | Documentos complexos, alta precisÃ£o semÃ¢ntica | MÃ©dia-Baixa | Muito Alta | Embedding Model |\n",
        "| **LateChunker** | Embedding primeiro, chunk depois | RAG avanÃ§ado, preservaÃ§Ã£o de contexto | Baixa | Muito Alta | Embedding Model |\n",
        "| **NeuralChunker** | Chunking baseado em redes neurais | Documentos nÃ£o estruturados, ML avanÃ§ado | Baixa | Muito Alta | Neural Model |\n",
        "| **SlumberChunker** | Chunking otimizado para embeddings | OtimizaÃ§Ã£o para embedding models | MÃ©dia | Alta | Embedding Model |\n",
        "| **CodeChunker** | Chunking especÃ­fico para cÃ³digo fonte | RepositÃ³rios de cÃ³digo, documentaÃ§Ã£o tÃ©cnica | Alta | MÃ©dia | NÃ£o |\n",
        "\n",
        "### ğŸ” **Detalhamento das EstratÃ©gias**\n",
        "\n",
        "#### **TokenChunker** ğŸš€\n",
        "- **Como funciona**: Divide texto em chunks de tamanho fixo baseado em contagem de tokens\n",
        "- **Vantagem**: Controle preciso para limites de API, processamento muito rÃ¡pido\n",
        "- **Ideal para**: OpenAI API, Claude API, modelos com limites rÃ­gidos de contexto\n",
        "\n",
        "#### **SentenceChunker** ğŸ“  \n",
        "- **Como funciona**: Agrupa sentenÃ§as em chunks respeitando limites semÃ¢nticos\n",
        "- **Vantagem**: Preserva integridade semÃ¢ntica, fÃ¡cil de implementar\n",
        "- **Ideal para**: Textos narrativos, documentos educacionais, artigos de blog\n",
        "\n",
        "#### **RecursiveChunker** ğŸŒ³\n",
        "- **Como funciona**: Aplica regras hierÃ¡rquicas (headers â†’ parÃ¡grafos â†’ sentenÃ§as)\n",
        "- **Vantagem**: Respeita estrutura do documento, configurÃ¡vel\n",
        "- **Ideal para**: Papers acadÃªmicos, documentaÃ§Ã£o tÃ©cnica, relatÃ³rios estruturados\n",
        "\n",
        "#### **SemanticChunker** ğŸ§ \n",
        "- **Como funciona**: Usa embeddings para agrupar conteÃºdo semanticamente similar\n",
        "- **Vantagem**: Chunks tematicamente coerentes, melhor para recuperaÃ§Ã£o\n",
        "- **Ideal para**: Documentos longos, conteÃºdo multi-tÃ³pico, knowledge bases\n",
        "\n",
        "#### **LateChunker** âš¡\n",
        "- **Como funciona**: Processa documento inteiro primeiro, depois chunking com contexto global\n",
        "- **Vantagem**: Preserva contexto mÃ¡ximo, estado da arte para RAG\n",
        "- **Ideal para**: RAG de alta qualidade, documentos complexos\n",
        "\n",
        "#### **SDPMChunker** ğŸ”„\n",
        "- **Como funciona**: Semantic Double-Pass Merge - duas passadas para otimizar semÃ¢ntica\n",
        "- **Vantagem**: MÃ¡xima precisÃ£o semÃ¢ntica, reduz perda de contexto\n",
        "- **Ideal para**: Documentos cientÃ­ficos complexos, anÃ¡lise legal, pesquisa mÃ©dica\n",
        "\n",
        "#### **NeuralChunker** ğŸ¤–\n",
        "- **Como funciona**: Redes neurais treinadas para identificar pontos ideais de divisÃ£o\n",
        "- **Vantagem**: Aprende padrÃµes complexos, adaptÃ¡vel a diferentes domÃ­nios\n",
        "- **Ideal para**: Textos nÃ£o estruturados, linguagem natural complexa\n",
        "\n",
        "#### **SlumberChunker** ğŸ˜´\n",
        "- **Como funciona**: Chunking otimizado especificamente para modelos de embedding\n",
        "- **Vantagem**: Maximiza qualidade dos embeddings resultantes\n",
        "- **Ideal para**: Sistemas de busca semÃ¢ntica, recomendaÃ§Ã£o de conteÃºdo\n",
        "\n",
        "#### **CodeChunker** ğŸ’»\n",
        "- **Como funciona**: Usa AST (Abstract Syntax Tree) para chunking inteligente de cÃ³digo\n",
        "- **Vantagem**: Preserva estrutura sintÃ¡tica, respeita funÃ§Ãµes e classes\n",
        "- **Ideal para**: DocumentaÃ§Ã£o de cÃ³digo, anÃ¡lise de repositÃ³rios, AI coding assistants\n",
        "\n",
        "### âš–ï¸ **Guia de SeleÃ§Ã£o RÃ¡pida**\n",
        "\n",
        "| Prioridade | Escolha | Justificativa |\n",
        "|------------|---------|---------------|\n",
        "| **Velocidade** | TokenChunker â†’ SentenceChunker â†’ CodeChunker | Performance mÃ¡xima |\n",
        "| **Qualidade RAG** | LateChunker â†’ SDPMChunker â†’ SemanticChunker | Contexto preservado |\n",
        "| **Simplicidade** | SentenceChunker â†’ TokenChunker â†’ RecursiveChunker | FÃ¡cil implementaÃ§Ã£o |\n",
        "| **Flexibilidade** | RecursiveChunker â†’ SemanticChunker â†’ NeuralChunker | Configurabilidade |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸš€ **ImplementaÃ§Ã£o PrÃ¡tica** <a id=\"implementation\"></a>\n",
        "\n",
        "### PreparaÃ§Ã£o dos Dados de Teste\n",
        "\n",
        "Vamos criar textos de exemplo para diferentes cenÃ¡rios:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Texto CientÃ­fico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dados de teste para diferentes cenÃ¡rios\n",
        "\n",
        "# Texto mÃ©dico/cientÃ­fico\n",
        "medical_text = \"\"\"\n",
        "## IntroduÃ§Ã£o\n",
        "\n",
        "A inteligÃªncia artificial (IA) estÃ¡ revolucionando o setor de saÃºde, especialmente no diagnÃ³stico mÃ©dico. \n",
        "Machine learning e deep learning permitem anÃ¡lises mais precisas de exames de imagem, como radiografias, tomografias e ressonÃ¢ncias magnÃ©ticas.\n",
        "\n",
        "## Metodologia\n",
        "\n",
        "Neste estudo, utilizamos uma rede neural convolucional (CNN) para classificar imagens de radiografias de tÃ³rax. \n",
        "O dataset contÃ©m 10.000 imagens categorizadas em trÃªs classes: normal, pneumonia e COVID-19.\n",
        "\n",
        "### PrÃ©-processamento\n",
        "\n",
        "As imagens foram redimensionadas para 224x224 pixels e normalizadas. Aplicamos tÃ©cnicas de data augmentation \n",
        "para aumentar a variabilidade dos dados de treinamento.\n",
        "\n",
        "## Resultados\n",
        "\n",
        "O modelo alcanÃ§ou uma acurÃ¡cia de 94.2% no conjunto de teste, superando mÃ©todos convencionais. \n",
        "A sensibilidade para detecÃ§Ã£o de pneumonia foi de 96.1%, e para COVID-19, 91.8%.\n",
        "\n",
        "## ConclusÃ£o\n",
        "\n",
        "Os resultados demonstram o potencial da IA no diagnÃ³stico mÃ©dico, oferecendo uma ferramenta valiosa \n",
        "para profissionais de saÃºde em cenÃ¡rios de alta demanda.\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Texto educacional/narrativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "educational_text = \"\"\"\n",
        "A histÃ³ria da programaÃ§Ã£o Ã© fascinante e cheia de inovaÃ§Ãµes. ComeÃ§ou na dÃ©cada de 1940 com os primeiros computadores mecÃ¢nicos.\n",
        "\n",
        "Ada Lovelace Ã© frequentemente considerada a primeira programadora da histÃ³ria. Ela escreveu o primeiro algoritmo \n",
        "destinado a ser processado por uma mÃ¡quina, especificamente para a MÃ¡quina AnalÃ­tica de Charles Babbage.\n",
        "\n",
        "Na dÃ©cada de 1950, surgiram as primeiras linguagens de programaÃ§Ã£o de alto nÃ­vel. FORTRAN foi desenvolvida \n",
        "pela IBM para cÃ¡lculos cientÃ­ficos. Logo depois, COBOL emergiu para aplicaÃ§Ãµes comerciais.\n",
        "\n",
        "Os anos 1960 trouxeram ALGOL e posteriormente PASCAL, que influenciaram profundamente o design de linguagens modernas. \n",
        "A programaÃ§Ã£o estruturada se tornou um paradigma dominante.\n",
        "\n",
        "Na dÃ©cada de 1970, a linguagem C foi criada por Dennis Ritchie, revolucionando a programaÃ§Ã£o de sistemas. \n",
        "Sua influÃªncia perdura atÃ© hoje em linguagens como C++, Java e Python.\n",
        "\n",
        "A era da programaÃ§Ã£o orientada a objetos comeÃ§ou com Smalltalk nos anos 1970, mas ganhou popularidade \n",
        "com C++ na dÃ©cada de 1980. Este paradigma mudou fundamentalmente como pensamos sobre software.\n",
        "\n",
        "Hoje, temos centenas de linguagens de programaÃ§Ã£o, cada uma projetada para diferentes propÃ³sitos: \n",
        "web development, mobile apps, machine learning, data science, e muito mais.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### CÃ³digo exemplo para CodeChunker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "code_text = \"\"\"\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\\\"\\\"\\\"\n",
        "RAG Application with Chonkie Integration\n",
        "Advanced document processing and retrieval system\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "import os\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "# Third-party imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Chonkie imports\n",
        "from chonkie import TokenChunker, SemanticChunker, RecursiveChunker\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class ChunkResult:\n",
        "    \\\"\\\"\\\"Data class for chunk results\\\"\\\"\\\"\n",
        "    text: str\n",
        "    chunk_id: str\n",
        "    token_count: int\n",
        "    embedding: Optional[np.ndarray] = None\n",
        "    metadata: Dict[str, Any] = None\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \\\"\\\"\\\"Advanced document processing with multiple chunking strategies\\\"\\\"\\\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.chunks_cache = {}\n",
        "        logger.info(f\"Initialized DocumentProcessor with model: {model_name}\")\n",
        "    \n",
        "    def process_document(self, \n",
        "                        text: str, \n",
        "                        chunker_type: str = \"recursive\",\n",
        "                        chunk_size: int = 512) -> List[ChunkResult]:\n",
        "        \\\"\\\"\\\"Process document with specified chunker\\\"\\\"\\\"\n",
        "        try:\n",
        "            # Select chunker based on type\n",
        "            chunker = self._get_chunker(chunker_type, chunk_size)\n",
        "            \n",
        "            # Generate chunks\n",
        "            chunks = chunker(text)\n",
        "            \n",
        "            # Process each chunk\n",
        "            results = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                chunk_result = ChunkResult(\n",
        "                    text=chunk.text,\n",
        "                    chunk_id=f\"{chunker_type}_{i}\",\n",
        "                    token_count=len(chunk.text.split()),\n",
        "                    embedding=self.model.encode(chunk.text),\n",
        "                    metadata={\"chunker_type\": chunker_type, \"index\": i}\n",
        "                )\n",
        "                results.append(chunk_result)\n",
        "            \n",
        "            logger.info(f\"Processed {len(results)} chunks using {chunker_type}\")\n",
        "            return results\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing document: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    def _get_chunker(self, chunker_type: str, chunk_size: int):\n",
        "        \\\"\\\"\\\"Factory method for chunkers\\\"\\\"\\\"\n",
        "        chunkers = {\n",
        "            \"token\": TokenChunker(chunk_size=chunk_size),\n",
        "            \"semantic\": SemanticChunker(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"),\n",
        "            \"recursive\": RecursiveChunker(chunk_size=chunk_size)\n",
        "        }\n",
        "        return chunkers.get(chunker_type, chunkers[\"recursive\"])\n",
        "\n",
        "def main():\n",
        "    \\\"\\\"\\\"Main execution function\\\"\\\"\\\"\n",
        "    processor = DocumentProcessor()\n",
        "    \n",
        "    # Example usage\n",
        "    sample_text = \"Your document text here...\"\n",
        "    chunks = processor.process_document(sample_text, \"semantic\")\n",
        "    \n",
        "    print(f\"Generated {len(chunks)} chunks\")\n",
        "    for chunk in chunks[:3]:  # Show first 3 chunks\n",
        "        print(f\"Chunk ID: {chunk.chunk_id}  \")\n",
        "        print(f\"Tokens: {chunk.token_count}\")\n",
        "        print(f\"Preview: {chunk.text[:100]}...\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 1. TokenChunker - Controle Preciso de Tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ConfiguraÃ§Ã£o do TokenChunker\n",
        "token_chunker = TokenChunker(\n",
        "    chunk_size=100,  # Limite de tokens por chunk\n",
        "    chunk_overlap=10  # Overlap entre chunks\n",
        ")\n",
        "\n",
        "# Processamento\n",
        "token_chunks = token_chunker(medical_text)\n",
        "\n",
        "# AnÃ¡lise dos resultados\n",
        "print(f\"âœ… Chunks gerados: {len(token_chunks)}\")\n",
        "print(f\"ğŸ“Š MÃ©dia de tokens por chunk: {np.mean([chunk.token_count for chunk in token_chunks]):.1f}\")\n",
        "print(f\"ğŸ”„ Overlap configurado: {token_chunker.chunk_overlap} tokens\")\n",
        "\n",
        "# VisualizaÃ§Ã£o dos primeiros chunks\n",
        "for i, chunk in enumerate(token_chunks[:3]):\n",
        "    print(f\"\\nğŸ“„ Chunk {i+1} (Tokens: {chunk.token_count}):\")\n",
        "    print(f\"â”œâ”€ Texto: {chunk.text[:100]}...\")\n",
        "    print(f\"â””â”€ ID: {chunk.chunk_id}\")\n",
        "\n",
        "# Caso de uso: PreparaÃ§Ã£o para API OpenAI\n",
        "print(f\"\\nğŸ¯ Uso PrÃ¡tico:\")\n",
        "print(f\"Todos os chunks respeitam o limite de {token_chunker.chunk_size} tokens\")\n",
        "print(f\"Perfeito para APIs com limite de contexto\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2. SentenceChunker - PreservaÃ§Ã£o SemÃ¢ntica Natural\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SentenceChunker - Preserva integridade semÃ¢ntica\n",
        "print(\"ğŸ“ SentenceChunker Implementation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ConfiguraÃ§Ã£o do SentenceChunker\n",
        "sentence_chunker = SentenceChunker(\n",
        "    sentences_per_chunk=3,  # NÃºmero de sentenÃ§as por chunk\n",
        "    chunk_overlap=1  # SobreposiÃ§Ã£o de sentenÃ§as\n",
        ")\n",
        "\n",
        "# Processamento\n",
        "sentence_chunks = sentence_chunker(educational_text)\n",
        "\n",
        "# AnÃ¡lise dos resultados\n",
        "print(f\"âœ… Chunks gerados: {len(sentence_chunks)}\")\n",
        "print(f\"ğŸ“Š MÃ©dia de sentenÃ§as por chunk: {np.mean([len(chunk.text.split('.')) for chunk in sentence_chunks]):.1f}\")\n",
        "\n",
        "# VisualizaÃ§Ã£o dos primeiros chunks\n",
        "for i, chunk in enumerate(sentence_chunks[:3]):\n",
        "    sentences_count = len([s for s in chunk.text.split('.') if s.strip()])\n",
        "    print(f\"\\nğŸ“„ Chunk {i+1} (SentenÃ§as: {sentences_count}):\")\n",
        "    print(f\"â”œâ”€ Texto: {chunk.text[:120]}...\")\n",
        "    print(f\"â””â”€ Preserva contexto semÃ¢ntico completo\")\n",
        "\n",
        "# ComparaÃ§Ã£o com TokenChunker\n",
        "print(f\"\\nğŸ” ComparaÃ§Ã£o:\")\n",
        "print(f\"â”œâ”€ TokenChunker: Corta no meio das sentenÃ§as\")\n",
        "print(f\"â””â”€ SentenceChunker: Preserva integridade semÃ¢ntica\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3. RecursiveChunker - Estrutura HierÃ¡rquica Inteligente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RecursiveChunker - Respeita estrutura hierÃ¡rquica\n",
        "print(\"ğŸŒ³ RecursiveChunker Implementation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ConfiguraÃ§Ã£o do RecursiveChunker\n",
        "recursive_chunker = RecursiveChunker(\n",
        "    chunk_size=200,  # Tamanho mÃ¡ximo do chunk\n",
        "    chunk_overlap=20,  # Overlap entre chunks\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Hierarquia de separadores\n",
        ")\n",
        "\n",
        "# Processamento\n",
        "recursive_chunks = recursive_chunker(medical_text)\n",
        "\n",
        "# AnÃ¡lise dos resultados\n",
        "print(f\"âœ… Chunks gerados: {len(recursive_chunks)}\")\n",
        "print(f\"ğŸ“Š MÃ©dia de caracteres por chunk: {np.mean([len(chunk.text) for chunk in recursive_chunks]):.1f}\")\n",
        "\n",
        "# VisualizaÃ§Ã£o dos primeiros chunks\n",
        "for i, chunk in enumerate(recursive_chunks[:3]):\n",
        "    print(f\"\\nğŸ“„ Chunk {i+1} (Caracteres: {len(chunk.text)}):\")\n",
        "    print(f\"â”œâ”€ Texto: {chunk.text[:100]}...\")\n",
        "    # Verificar se mantÃ©m estrutura\n",
        "    has_headers = any(line.startswith(\"#\") for line in chunk.text.split(\"\\n\"))\n",
        "    print(f\"â””â”€ MantÃ©m headers: {'âœ…' if has_headers else 'âŒ'}\")\n",
        "\n",
        "# AnÃ¡lise da estrutura preservada\n",
        "print(f\"\\nğŸ” Estrutura Preservada:\")\n",
        "structure_analysis = []\n",
        "for chunk in recursive_chunks:\n",
        "    has_title = any(line.startswith(\"##\") for line in chunk.text.split(\"\\n\"))\n",
        "    has_subtitle = any(line.startswith(\"###\") for line in chunk.text.split(\"\\n\"))\n",
        "    structure_analysis.append({\"title\": has_title, \"subtitle\": has_subtitle})\n",
        "\n",
        "titles_preserved = sum(1 for s in structure_analysis if s[\"title\"])\n",
        "subtitles_preserved = sum(1 for s in structure_analysis if s[\"subtitle\"])\n",
        "\n",
        "print(f\"â”œâ”€ Chunks com tÃ­tulos principais: {titles_preserved}\")\n",
        "print(f\"â””â”€ Chunks com subtÃ­tulos: {subtitles_preserved}\")\n",
        "print(f\"ğŸ“ˆ Ideal para documentos estruturados como papers acadÃªmicos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4. CodeChunker - Processamento Inteligente de CÃ³digo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CodeChunker - Especializado em cÃ³digo fonte\n",
        "print(\"ğŸ’» CodeChunker Implementation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ConfiguraÃ§Ã£o do CodeChunker\n",
        "code_chunker = CodeChunker(\n",
        "    chunk_size=500,  # Tamanho mÃ¡ximo do chunk\n",
        "    chunk_overlap=50,  # Overlap entre chunks\n",
        "    language=\"python\"  # Linguagem especÃ­fica\n",
        ")\n",
        "\n",
        "# Processamento\n",
        "code_chunks = code_chunker(code_text)\n",
        "\n",
        "# AnÃ¡lise dos resultados\n",
        "print(f\"âœ… Chunks gerados: {len(code_chunks)}\")\n",
        "print(f\"ğŸ“Š MÃ©dia de caracteres por chunk: {np.mean([len(chunk.text) for chunk in code_chunks]):.1f}\")\n",
        "\n",
        "# VisualizaÃ§Ã£o dos primeiros chunks\n",
        "for i, chunk in enumerate(code_chunks[:3]):\n",
        "    lines = chunk.text.split('\\n')\n",
        "    print(f\"\\nğŸ“„ Chunk {i+1} (Linhas: {len(lines)}):\")\n",
        "    \n",
        "    # AnÃ¡lise do conteÃºdo\n",
        "    has_imports = any('import' in line for line in lines)\n",
        "    has_functions = any('def ' in line for line in lines)\n",
        "    has_classes = any('class ' in line for line in lines)\n",
        "    has_comments = any(line.strip().startswith('#') for line in lines)\n",
        "    \n",
        "    print(f\"â”œâ”€ Imports: {'âœ…' if has_imports else 'âŒ'}\")\n",
        "    print(f\"â”œâ”€ FunÃ§Ãµes: {'âœ…' if has_functions else 'âŒ'}\")\n",
        "    print(f\"â”œâ”€ Classes: {'âœ…' if has_classes else 'âŒ'}\")\n",
        "    print(f\"â”œâ”€ ComentÃ¡rios: {'âœ…' if has_comments else 'âŒ'}\")\n",
        "    print(f\"â””â”€ Preview: {chunk.text[:80]}...\")\n",
        "\n",
        "# AnÃ¡lise da preservaÃ§Ã£o sintÃ¡tica\n",
        "print(f\"\\nğŸ” AnÃ¡lise SintÃ¡tica:\")\n",
        "syntactic_elements = {\n",
        "    'imports': 0,\n",
        "    'functions': 0,\n",
        "    'classes': 0,\n",
        "    'docstrings': 0\n",
        "}\n",
        "\n",
        "for chunk in code_chunks:\n",
        "    lines = chunk.text.split('\\n')\n",
        "    syntactic_elements['imports'] += sum(1 for line in lines if 'import' in line)\n",
        "    syntactic_elements['functions'] += sum(1 for line in lines if 'def ' in line.strip())\n",
        "    syntactic_elements['classes'] += sum(1 for line in lines if 'class ' in line.strip())\n",
        "    syntactic_elements['docstrings'] += sum(1 for line in lines if '\\\"\\\"\\\"' in line)\n",
        "\n",
        "for element, count in syntactic_elements.items():\n",
        "    print(f\"â”œâ”€ {element.title()}: {count}\")\n",
        "\n",
        "print(f\"â””â”€ ğŸ¯ Preserva estrutura sintÃ¡tica do cÃ³digo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ”§ **Troubleshooting e LimitaÃ§Ãµes** <a id=\"troubleshooting\"></a>\n",
        "\n",
        "### Problemas Comuns e SoluÃ§Ãµes\n",
        "\n",
        "#### âš ï¸ **Chunkers AvanÃ§ados (Semantic, SDPM, Late, Neural)**\n",
        "```python\n",
        "# Problema: Requer modelos de embedding ou redes neurais\n",
        "# SoluÃ§Ã£o: Instalar dependÃªncias apropriadas\n",
        "\n",
        "# Para SemanticChunker\n",
        "# uv add sentence-transformers\n",
        "\n",
        "# Para LateChunker\n",
        "# uv add transformers torch\n",
        "\n",
        "# Para NeuralChunker\n",
        "# uv add torch transformers datasets\n",
        "```\n",
        "\n",
        "#### ğŸ› **Problemas de Performance**\n",
        "- **TokenChunker**: Pode cortar no meio de palavras importantes\n",
        "- **SemanticChunker**: Lento para documentos grandes\n",
        "- **LateChunker**: Requer muita memÃ³ria RAM\n",
        "\n",
        "#### ğŸ“Š **LimitaÃ§Ãµes por Chunker**\n",
        "\n",
        "| Chunker | LimitaÃ§Ã£o Principal | SoluÃ§Ã£o |\n",
        "|---------|---------------------|---------|\n",
        "| **TokenChunker** | Quebra no meio de sentenÃ§as | Use com overlap adequado |\n",
        "| **SentenceChunker** | SentenÃ§as muito longas | Configure max_chunk_size |\n",
        "| **RecursiveChunker** | Dependente de separadores | Customize separators |\n",
        "| **SemanticChunker** | Lento para textos grandes | Use em documentos < 50KB |\n",
        "| **CodeChunker** | EspecÃ­fico para linguagens suportadas | Verifique support languages |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ“š **ReferÃªncias e Recursos**\n",
        "\n",
        "### DocumentaÃ§Ã£o Oficial\n",
        "- **Website**: [chonkie.ai](https://chonkie.ai)\n",
        "- **DocumentaÃ§Ã£o**: [docs.chonkie.ai](https://docs.chonkie.ai)\n",
        "- **GitHub**: [github.com/chonkie-ai/chonkie](https://github.com/chonkie-ai/chonkie)\n",
        "- **PyPI**: [pypi.org/project/chonkie](https://pypi.org/project/chonkie)\n",
        "\n",
        "### Recursos Adicionais\n",
        "- **RAG Best Practices**: [Retrieval-Augmented Generation Guide](https://example.com/rag-guide)\n",
        "- **Embedding Models**: [Sentence Transformers](https://www.sbert.net/)\n",
        "- **Tokenization**: [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/)\n",
        "\n",
        "---\n",
        "\n",
        "JoÃ£o Gabriel Lima  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
