{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Chonkie.ai - Advanced Text Chunking for RAG Applications\n",
        "\n",
        "`by Jo√£o Gabriel Lima`\n",
        "\n",
        "## üéØ **Objetivo**\n",
        "\n",
        "Este notebook explora **Chonkie.ai**, uma biblioteca Python especializada em chunking de texto para aplica√ß√µes RAG (Retrieval-Augmented Generation). Vamos analisar as 9 estrat√©gias de chunking dispon√≠veis, implementar casos de uso pr√°ticos e avaliar performance.\n",
        "\n",
        "## üìã **Conte√∫do**\n",
        "\n",
        "1. [Introdu√ß√£o e Instala√ß√£o](#intro)\n",
        "2. [Configura√ß√£o e Imports](#config)\n",
        "3. [An√°lise Comparativa dos Chunkers](#comparison)\n",
        "4. [Implementa√ß√£o Pr√°tica](#implementation)\n",
        "5. [Casos de Uso Espec√≠ficos](#use-cases)\n",
        "6. [An√°lise de Performance](#performance)\n",
        "7. [Troubleshooting e Limita√ß√µes](#troubleshooting)\n",
        "8. [Conclus√µes e Recomenda√ß√µes](#conclusions)\n",
        "\n",
        "---\n",
        "\n",
        "## üîß **Pr√©-requisitos**\n",
        "\n",
        "- Python 3.8+\n",
        "- Conhecimento b√°sico de NLP e RAG\n",
        "- Familiaridade com embeddings e modelos de linguagem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üì¶ **Instala√ß√£o** <a id=\"intro\"></a>\n",
        "\n",
        "### Instala√ß√£o B√°sica\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "shell"
        }
      },
      "source": [
        "# Instala√ß√£o b√°sica\n",
        "uv add chonkie\n",
        "\n",
        "# Instala√ß√£o completa com todos os recursos\n",
        "uv add \"chonkie[all]\"\n",
        "\n",
        "# Depend√™ncias adicionais para testes abrangentes\n",
        "uv add tiktoken sentence-transformers transformers torch numpy pandas matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "uv add chonkie"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ‚öôÔ∏è **Configura√ß√£o e Imports** <a id=\"config\"></a>\n",
        "\n",
        "### Imports Essenciais\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core Chonkie imports\n",
        "from chonkie import (\n",
        "    TokenChunker,\n",
        "    SentenceChunker, \n",
        "    RecursiveChunker,\n",
        "    SemanticChunker,\n",
        "    SDPMChunker,\n",
        "    LateChunker,\n",
        "    NeuralChunker,\n",
        "    SlumberChunker,\n",
        "    CodeChunker\n",
        ")\n",
        "\n",
        "# Additional utilities\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "from typing import List, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Imports realizados com sucesso!\")\n",
        "print(f\"üìä Chunkers dispon√≠veis: {len([TokenChunker, SentenceChunker, RecursiveChunker, SemanticChunker, SDPMChunker, LateChunker, NeuralChunker, SlumberChunker, CodeChunker])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n",
        "## üîç **An√°lise Comparativa dos Chunkers** <a id=\"comparison\"></a>\n",
        "\n",
        "### Chunkers Comparison Table\n",
        "\n",
        "A tabela abaixo detalha as principais diferen√ßas, casos de uso e funcionamento de cada chunker dispon√≠vel no Chonkie.ai:\n",
        "\n",
        "| Chunker | Estrat√©gia | Caso de Uso Ideal | Performance | Complexidade | Requer Modelo |\n",
        "|---------|------------|-------------------|-------------|--------------|---------------|\n",
        "| **TokenChunker** | Divis√£o por tokens fixos | APIs com limites de tokens, controle preciso | Muito Alta | Baixa | Tokenizer |\n",
        "| **SentenceChunker** | Divis√£o por limites de senten√ßa | Documentos bem estruturados, textos narrativos | Alta | Baixa | N√£o |\n",
        "| **RecursiveChunker** | Divis√£o hier√°rquica com regras customiz√°veis | Documentos hier√°rquicos, papers acad√™micos | M√©dia-Alta | M√©dia | N√£o |\n",
        "| **SemanticChunker** | Agrupamento por similaridade sem√¢ntica | Documentos longos, conte√∫do heterog√™neo | M√©dia | Alta | Embedding Model |\n",
        "| **SDPMChunker** | Semantic Double-Pass Merge | Documentos complexos, alta precis√£o sem√¢ntica | M√©dia-Baixa | Muito Alta | Embedding Model |\n",
        "| **LateChunker** | Embedding primeiro, chunk depois | RAG avan√ßado, preserva√ß√£o de contexto | Baixa | Muito Alta | Embedding Model |\n",
        "| **NeuralChunker** | Chunking baseado em redes neurais | Documentos n√£o estruturados, ML avan√ßado | Baixa | Muito Alta | Neural Model |\n",
        "| **SlumberChunker** | Chunking otimizado para embeddings | Otimiza√ß√£o para embedding models | M√©dia | Alta | Embedding Model |\n",
        "| **CodeChunker** | Chunking espec√≠fico para c√≥digo fonte | Reposit√≥rios de c√≥digo, documenta√ß√£o t√©cnica | Alta | M√©dia | N√£o |\n",
        "\n",
        "### üîç **Detalhamento das Estrat√©gias**\n",
        "\n",
        "#### **TokenChunker** üöÄ\n",
        "- **Como funciona**: Divide texto em chunks de tamanho fixo baseado em contagem de tokens\n",
        "- **Vantagem**: Controle preciso para limites de API, processamento muito r√°pido\n",
        "- **Ideal para**: OpenAI API, Claude API, modelos com limites r√≠gidos de contexto\n",
        "\n",
        "#### **SentenceChunker** üìù  \n",
        "- **Como funciona**: Agrupa senten√ßas em chunks respeitando limites sem√¢nticos\n",
        "- **Vantagem**: Preserva integridade sem√¢ntica, f√°cil de implementar\n",
        "- **Ideal para**: Textos narrativos, documentos educacionais, artigos de blog\n",
        "\n",
        "#### **RecursiveChunker** üå≥\n",
        "- **Como funciona**: Aplica regras hier√°rquicas (headers ‚Üí par√°grafos ‚Üí senten√ßas)\n",
        "- **Vantagem**: Respeita estrutura do documento, configur√°vel\n",
        "- **Ideal para**: Papers acad√™micos, documenta√ß√£o t√©cnica, relat√≥rios estruturados\n",
        "\n",
        "#### **SemanticChunker** üß†\n",
        "- **Como funciona**: Usa embeddings para agrupar conte√∫do semanticamente similar\n",
        "- **Vantagem**: Chunks tematicamente coerentes, melhor para recupera√ß√£o\n",
        "- **Ideal para**: Documentos longos, conte√∫do multi-t√≥pico, knowledge bases\n",
        "\n",
        "#### **LateChunker** ‚ö°\n",
        "- **Como funciona**: Processa documento inteiro primeiro, depois chunking com contexto global\n",
        "- **Vantagem**: Preserva contexto m√°ximo, estado da arte para RAG\n",
        "- **Ideal para**: RAG de alta qualidade, documentos complexos\n",
        "\n",
        "#### **SDPMChunker** üîÑ\n",
        "- **Como funciona**: Semantic Double-Pass Merge - duas passadas para otimizar sem√¢ntica\n",
        "- **Vantagem**: M√°xima precis√£o sem√¢ntica, reduz perda de contexto\n",
        "- **Ideal para**: Documentos cient√≠ficos complexos, an√°lise legal, pesquisa m√©dica\n",
        "\n",
        "#### **NeuralChunker** ü§ñ\n",
        "- **Como funciona**: Redes neurais treinadas para identificar pontos ideais de divis√£o\n",
        "- **Vantagem**: Aprende padr√µes complexos, adapt√°vel a diferentes dom√≠nios\n",
        "- **Ideal para**: Textos n√£o estruturados, linguagem natural complexa\n",
        "\n",
        "#### **SlumberChunker** üò¥\n",
        "- **Como funciona**: Chunking otimizado especificamente para modelos de embedding\n",
        "- **Vantagem**: Maximiza qualidade dos embeddings resultantes\n",
        "- **Ideal para**: Sistemas de busca sem√¢ntica, recomenda√ß√£o de conte√∫do\n",
        "\n",
        "#### **CodeChunker** üíª\n",
        "- **Como funciona**: Usa AST (Abstract Syntax Tree) para chunking inteligente de c√≥digo\n",
        "- **Vantagem**: Preserva estrutura sint√°tica, respeita fun√ß√µes e classes\n",
        "- **Ideal para**: Documenta√ß√£o de c√≥digo, an√°lise de reposit√≥rios, AI coding assistants\n",
        "\n",
        "### ‚öñÔ∏è **Guia de Sele√ß√£o R√°pida**\n",
        "\n",
        "| Prioridade | Escolha | Justificativa |\n",
        "|------------|---------|---------------|\n",
        "| **Velocidade** | TokenChunker ‚Üí SentenceChunker ‚Üí CodeChunker | Performance m√°xima |\n",
        "| **Qualidade RAG** | LateChunker ‚Üí SDPMChunker ‚Üí SemanticChunker | Contexto preservado |\n",
        "| **Simplicidade** | SentenceChunker ‚Üí TokenChunker ‚Üí RecursiveChunker | F√°cil implementa√ß√£o |\n",
        "| **Flexibilidade** | RecursiveChunker ‚Üí SemanticChunker ‚Üí NeuralChunker | Configurabilidade |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üöÄ **Implementa√ß√£o Pr√°tica** <a id=\"implementation\"></a>\n",
        "\n",
        "### Prepara√ß√£o dos Dados de Teste\n",
        "\n",
        "Vamos criar textos de exemplo para diferentes cen√°rios:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Texto Cient√≠fico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dados de teste para diferentes cen√°rios\n",
        "\n",
        "# Texto m√©dico/cient√≠fico\n",
        "medical_text = \"\"\"\n",
        "## Introdu√ß√£o\n",
        "\n",
        "A intelig√™ncia artificial (IA) est√° revolucionando o setor de sa√∫de, especialmente no diagn√≥stico m√©dico. \n",
        "Machine learning e deep learning permitem an√°lises mais precisas de exames de imagem, como radiografias, tomografias e resson√¢ncias magn√©ticas.\n",
        "\n",
        "## Metodologia\n",
        "\n",
        "Neste estudo, utilizamos uma rede neural convolucional (CNN) para classificar imagens de radiografias de t√≥rax. \n",
        "O dataset cont√©m 10.000 imagens categorizadas em tr√™s classes: normal, pneumonia e COVID-19.\n",
        "\n",
        "### Pr√©-processamento\n",
        "\n",
        "As imagens foram redimensionadas para 224x224 pixels e normalizadas. Aplicamos t√©cnicas de data augmentation \n",
        "para aumentar a variabilidade dos dados de treinamento.\n",
        "\n",
        "## Resultados\n",
        "\n",
        "O modelo alcan√ßou uma acur√°cia de 94.2% no conjunto de teste, superando m√©todos convencionais. \n",
        "A sensibilidade para detec√ß√£o de pneumonia foi de 96.1%, e para COVID-19, 91.8%.\n",
        "\n",
        "## Conclus√£o\n",
        "\n",
        "Os resultados demonstram o potencial da IA no diagn√≥stico m√©dico, oferecendo uma ferramenta valiosa \n",
        "para profissionais de sa√∫de em cen√°rios de alta demanda.\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Texto educacional/narrativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "educational_text = \"\"\"\n",
        "A hist√≥ria da programa√ß√£o √© fascinante e cheia de inova√ß√µes. Come√ßou na d√©cada de 1940 com os primeiros computadores mec√¢nicos.\n",
        "\n",
        "Ada Lovelace √© frequentemente considerada a primeira programadora da hist√≥ria. Ela escreveu o primeiro algoritmo \n",
        "destinado a ser processado por uma m√°quina, especificamente para a M√°quina Anal√≠tica de Charles Babbage.\n",
        "\n",
        "Na d√©cada de 1950, surgiram as primeiras linguagens de programa√ß√£o de alto n√≠vel. FORTRAN foi desenvolvida \n",
        "pela IBM para c√°lculos cient√≠ficos. Logo depois, COBOL emergiu para aplica√ß√µes comerciais.\n",
        "\n",
        "Os anos 1960 trouxeram ALGOL e posteriormente PASCAL, que influenciaram profundamente o design de linguagens modernas. \n",
        "A programa√ß√£o estruturada se tornou um paradigma dominante.\n",
        "\n",
        "Na d√©cada de 1970, a linguagem C foi criada por Dennis Ritchie, revolucionando a programa√ß√£o de sistemas. \n",
        "Sua influ√™ncia perdura at√© hoje em linguagens como C++, Java e Python.\n",
        "\n",
        "A era da programa√ß√£o orientada a objetos come√ßou com Smalltalk nos anos 1970, mas ganhou popularidade \n",
        "com C++ na d√©cada de 1980. Este paradigma mudou fundamentalmente como pensamos sobre software.\n",
        "\n",
        "Hoje, temos centenas de linguagens de programa√ß√£o, cada uma projetada para diferentes prop√≥sitos: \n",
        "web development, mobile apps, machine learning, data science, e muito mais.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### C√≥digo exemplo para CodeChunker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "code_text = \"\"\"\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\\\"\\\"\\\"\n",
        "RAG Application with Chonkie Integration\n",
        "Advanced document processing and retrieval system\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "import os\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "# Third-party imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Chonkie imports\n",
        "from chonkie import TokenChunker, SemanticChunker, RecursiveChunker\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class ChunkResult:\n",
        "    \\\"\\\"\\\"Data class for chunk results\\\"\\\"\\\"\n",
        "    text: str\n",
        "    chunk_id: str\n",
        "    token_count: int\n",
        "    embedding: Optional[np.ndarray] = None\n",
        "    metadata: Dict[str, Any] = None\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \\\"\\\"\\\"Advanced document processing with multiple chunking strategies\\\"\\\"\\\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.chunks_cache = {}\n",
        "        logger.info(f\"Initialized DocumentProcessor with model: {model_name}\")\n",
        "    \n",
        "    def process_document(self, \n",
        "                        text: str, \n",
        "                        chunker_type: str = \"recursive\",\n",
        "                        chunk_size: int = 512) -> List[ChunkResult]:\n",
        "        \\\"\\\"\\\"Process document with specified chunker\\\"\\\"\\\"\n",
        "        try:\n",
        "            # Select chunker based on type\n",
        "            chunker = self._get_chunker(chunker_type, chunk_size)\n",
        "            \n",
        "            # Generate chunks\n",
        "            chunks = chunker(text)\n",
        "            \n",
        "            # Process each chunk\n",
        "            results = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                chunk_result = ChunkResult(\n",
        "                    text=chunk.text,\n",
        "                    chunk_id=f\"{chunker_type}_{i}\",\n",
        "                    token_count=len(chunk.text.split()),\n",
        "                    embedding=self.model.encode(chunk.text),\n",
        "                    metadata={\"chunker_type\": chunker_type, \"index\": i}\n",
        "                )\n",
        "                results.append(chunk_result)\n",
        "            \n",
        "            logger.info(f\"Processed {len(results)} chunks using {chunker_type}\")\n",
        "            return results\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing document: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    def _get_chunker(self, chunker_type: str, chunk_size: int):\n",
        "        \\\"\\\"\\\"Factory method for chunkers\\\"\\\"\\\"\n",
        "        chunkers = {\n",
        "            \"token\": TokenChunker(chunk_size=chunk_size),\n",
        "            \"semantic\": SemanticChunker(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"),\n",
        "            \"recursive\": RecursiveChunker(chunk_size=chunk_size)\n",
        "        }\n",
        "        return chunkers.get(chunker_type, chunkers[\"recursive\"])\n",
        "\n",
        "def main():\n",
        "    \\\"\\\"\\\"Main execution function\\\"\\\"\\\"\n",
        "    processor = DocumentProcessor()\n",
        "    \n",
        "    # Example usage\n",
        "    sample_text = \"Your document text here...\"\n",
        "    chunks = processor.process_document(sample_text, \"semantic\")\n",
        "    \n",
        "    print(f\"Generated {len(chunks)} chunks\")\n",
        "    for chunk in chunks[:3]:  # Show first 3 chunks\n",
        "        print(f\"Chunk ID: {chunk.chunk_id}  \")\n",
        "        print(f\"Tokens: {chunk.token_count}\")\n",
        "        print(f\"Preview: {chunk.text[:100]}...\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 1. TokenChunker - Controle Preciso de Tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configura√ß√£o do TokenChunker\n",
        "token_chunker = TokenChunker(\n",
        "    chunk_size=100,  # Limite de tokens por chunk\n",
        "    chunk_overlap=10  # Overlap entre chunks\n",
        ")\n",
        "\n",
        "# Processamento\n",
        "token_chunks = token_chunker(medical_text)\n",
        "\n",
        "# An√°lise dos resultados\n",
        "print(f\"‚úÖ Chunks gerados: {len(token_chunks)}\")\n",
        "print(f\"üìä M√©dia de tokens por chunk: {np.mean([chunk.token_count for chunk in token_chunks]):.1f}\")\n",
        "print(f\"üîÑ Overlap configurado: {token_chunker.chunk_overlap} tokens\")\n",
        "\n",
        "# Visualiza√ß√£o dos primeiros chunks\n",
        "for i, chunk in enumerate(token_chunks[:3]):\n",
        "    print(f\"\\nüìÑ Chunk {i+1} (Tokens: {chunk.token_count}):\")\n",
        "    print(f\"‚îú‚îÄ Texto: {chunk.text[:100]}...\")\n",
        "    print(f\"‚îî‚îÄ ID: {chunk.chunk_id}\")\n",
        "\n",
        "# Caso de uso: Prepara√ß√£o para API OpenAI\n",
        "print(f\"\\nüéØ Uso Pr√°tico:\")\n",
        "print(f\"Todos os chunks respeitam o limite de {token_chunker.chunk_size} tokens\")\n",
        "print(f\"Perfeito para APIs com limite de contexto\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2. SentenceChunker - Preserva√ß√£o Sem√¢ntica Natural\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SentenceChunker - Preserva integridade sem√¢ntica\n",
        "print(\"üìù SentenceChunker Implementation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Configura√ß√£o do SentenceChunker\n",
        "sentence_chunker = SentenceChunker(\n",
        "    sentences_per_chunk=3,  # N√∫mero de senten√ßas por chunk\n",
        "    chunk_overlap=1  # Sobreposi√ß√£o de senten√ßas\n",
        ")\n",
        "\n",
        "# Processamento\n",
        "sentence_chunks = sentence_chunker(educational_text)\n",
        "\n",
        "# An√°lise dos resultados\n",
        "print(f\"‚úÖ Chunks gerados: {len(sentence_chunks)}\")\n",
        "print(f\"üìä M√©dia de senten√ßas por chunk: {np.mean([len(chunk.text.split('.')) for chunk in sentence_chunks]):.1f}\")\n",
        "\n",
        "# Visualiza√ß√£o dos primeiros chunks\n",
        "for i, chunk in enumerate(sentence_chunks[:3]):\n",
        "    sentences_count = len([s for s in chunk.text.split('.') if s.strip()])\n",
        "    print(f\"\\nüìÑ Chunk {i+1} (Senten√ßas: {sentences_count}):\")\n",
        "    print(f\"‚îú‚îÄ Texto: {chunk.text[:120]}...\")\n",
        "    print(f\"‚îî‚îÄ Preserva contexto sem√¢ntico completo\")\n",
        "\n",
        "# Compara√ß√£o com TokenChunker\n",
        "print(f\"\\nüîç Compara√ß√£o:\")\n",
        "print(f\"‚îú‚îÄ TokenChunker: Corta no meio das senten√ßas\")\n",
        "print(f\"‚îî‚îÄ SentenceChunker: Preserva integridade sem√¢ntica\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3. RecursiveChunker - Estrutura Hier√°rquica Inteligente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RecursiveChunker - Respeita estrutura hier√°rquica\n",
        "print(\"üå≥ RecursiveChunker Implementation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Configura√ß√£o do RecursiveChunker\n",
        "recursive_chunker = RecursiveChunker(\n",
        "    chunk_size=200,  # Tamanho m√°ximo do chunk\n",
        "    chunk_overlap=20,  # Overlap entre chunks\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Hierarquia de separadores\n",
        ")\n",
        "\n",
        "# Processamento\n",
        "recursive_chunks = recursive_chunker(medical_text)\n",
        "\n",
        "# An√°lise dos resultados\n",
        "print(f\"‚úÖ Chunks gerados: {len(recursive_chunks)}\")\n",
        "print(f\"üìä M√©dia de caracteres por chunk: {np.mean([len(chunk.text) for chunk in recursive_chunks]):.1f}\")\n",
        "\n",
        "# Visualiza√ß√£o dos primeiros chunks\n",
        "for i, chunk in enumerate(recursive_chunks[:3]):\n",
        "    print(f\"\\nüìÑ Chunk {i+1} (Caracteres: {len(chunk.text)}):\")\n",
        "    print(f\"‚îú‚îÄ Texto: {chunk.text[:100]}...\")\n",
        "    # Verificar se mant√©m estrutura\n",
        "    has_headers = any(line.startswith(\"#\") for line in chunk.text.split(\"\\n\"))\n",
        "    print(f\"‚îî‚îÄ Mant√©m headers: {'‚úÖ' if has_headers else '‚ùå'}\")\n",
        "\n",
        "# An√°lise da estrutura preservada\n",
        "print(f\"\\nüîç Estrutura Preservada:\")\n",
        "structure_analysis = []\n",
        "for chunk in recursive_chunks:\n",
        "    has_title = any(line.startswith(\"##\") for line in chunk.text.split(\"\\n\"))\n",
        "    has_subtitle = any(line.startswith(\"###\") for line in chunk.text.split(\"\\n\"))\n",
        "    structure_analysis.append({\"title\": has_title, \"subtitle\": has_subtitle})\n",
        "\n",
        "titles_preserved = sum(1 for s in structure_analysis if s[\"title\"])\n",
        "subtitles_preserved = sum(1 for s in structure_analysis if s[\"subtitle\"])\n",
        "\n",
        "print(f\"‚îú‚îÄ Chunks com t√≠tulos principais: {titles_preserved}\")\n",
        "print(f\"‚îî‚îÄ Chunks com subt√≠tulos: {subtitles_preserved}\")\n",
        "print(f\"üìà Ideal para documentos estruturados como papers acad√™micos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4. CodeChunker - Processamento Inteligente de C√≥digo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CodeChunker - Especializado em c√≥digo fonte\n",
        "print(\"üíª CodeChunker Implementation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Configura√ß√£o do CodeChunker\n",
        "code_chunker = CodeChunker(\n",
        "    chunk_size=500,  # Tamanho m√°ximo do chunk\n",
        "    chunk_overlap=50,  # Overlap entre chunks\n",
        "    language=\"python\"  # Linguagem espec√≠fica\n",
        ")\n",
        "\n",
        "# Processamento\n",
        "code_chunks = code_chunker(code_text)\n",
        "\n",
        "# An√°lise dos resultados\n",
        "print(f\"‚úÖ Chunks gerados: {len(code_chunks)}\")\n",
        "print(f\"üìä M√©dia de caracteres por chunk: {np.mean([len(chunk.text) for chunk in code_chunks]):.1f}\")\n",
        "\n",
        "# Visualiza√ß√£o dos primeiros chunks\n",
        "for i, chunk in enumerate(code_chunks[:3]):\n",
        "    lines = chunk.text.split('\\n')\n",
        "    print(f\"\\nüìÑ Chunk {i+1} (Linhas: {len(lines)}):\")\n",
        "    \n",
        "    # An√°lise do conte√∫do\n",
        "    has_imports = any('import' in line for line in lines)\n",
        "    has_functions = any('def ' in line for line in lines)\n",
        "    has_classes = any('class ' in line for line in lines)\n",
        "    has_comments = any(line.strip().startswith('#') for line in lines)\n",
        "    \n",
        "    print(f\"‚îú‚îÄ Imports: {'‚úÖ' if has_imports else '‚ùå'}\")\n",
        "    print(f\"‚îú‚îÄ Fun√ß√µes: {'‚úÖ' if has_functions else '‚ùå'}\")\n",
        "    print(f\"‚îú‚îÄ Classes: {'‚úÖ' if has_classes else '‚ùå'}\")\n",
        "    print(f\"‚îú‚îÄ Coment√°rios: {'‚úÖ' if has_comments else '‚ùå'}\")\n",
        "    print(f\"‚îî‚îÄ Preview: {chunk.text[:80]}...\")\n",
        "\n",
        "# An√°lise da preserva√ß√£o sint√°tica\n",
        "print(f\"\\nüîç An√°lise Sint√°tica:\")\n",
        "syntactic_elements = {\n",
        "    'imports': 0,\n",
        "    'functions': 0,\n",
        "    'classes': 0,\n",
        "    'docstrings': 0\n",
        "}\n",
        "\n",
        "for chunk in code_chunks:\n",
        "    lines = chunk.text.split('\\n')\n",
        "    syntactic_elements['imports'] += sum(1 for line in lines if 'import' in line)\n",
        "    syntactic_elements['functions'] += sum(1 for line in lines if 'def ' in line.strip())\n",
        "    syntactic_elements['classes'] += sum(1 for line in lines if 'class ' in line.strip())\n",
        "    syntactic_elements['docstrings'] += sum(1 for line in lines if '\\\"\\\"\\\"' in line)\n",
        "\n",
        "for element, count in syntactic_elements.items():\n",
        "    print(f\"‚îú‚îÄ {element.title()}: {count}\")\n",
        "\n",
        "print(f\"‚îî‚îÄ üéØ Preserva estrutura sint√°tica do c√≥digo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîß **Troubleshooting e Limita√ß√µes** <a id=\"troubleshooting\"></a>\n",
        "\n",
        "### Problemas Comuns e Solu√ß√µes\n",
        "\n",
        "#### ‚ö†Ô∏è **Chunkers Avan√ßados (Semantic, SDPM, Late, Neural)**\n",
        "```python\n",
        "# Problema: Requer modelos de embedding ou redes neurais\n",
        "# Solu√ß√£o: Instalar depend√™ncias apropriadas\n",
        "\n",
        "# Para SemanticChunker\n",
        "# uv add sentence-transformers\n",
        "\n",
        "# Para LateChunker\n",
        "# uv add transformers torch\n",
        "\n",
        "# Para NeuralChunker\n",
        "# uv add torch transformers datasets\n",
        "```\n",
        "\n",
        "#### üêõ **Problemas de Performance**\n",
        "- **TokenChunker**: Pode cortar no meio de palavras importantes\n",
        "- **SemanticChunker**: Lento para documentos grandes\n",
        "- **LateChunker**: Requer muita mem√≥ria RAM\n",
        "\n",
        "#### üìä **Limita√ß√µes por Chunker**\n",
        "\n",
        "| Chunker | Limita√ß√£o Principal | Solu√ß√£o |\n",
        "|---------|---------------------|---------|\n",
        "| **TokenChunker** | Quebra no meio de senten√ßas | Use com overlap adequado |\n",
        "| **SentenceChunker** | Senten√ßas muito longas | Configure max_chunk_size |\n",
        "| **RecursiveChunker** | Dependente de separadores | Customize separators |\n",
        "| **SemanticChunker** | Lento para textos grandes | Use em documentos < 50KB |\n",
        "| **CodeChunker** | Espec√≠fico para linguagens suportadas | Verifique support languages |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìö **Refer√™ncias e Recursos**\n",
        "\n",
        "### Documenta√ß√£o Oficial\n",
        "- **Website**: [chonkie.ai](https://chonkie.ai)\n",
        "- **Documenta√ß√£o**: [docs.chonkie.ai](https://docs.chonkie.ai)\n",
        "- **GitHub**: [github.com/chonkie-ai/chonkie](https://github.com/chonkie-ai/chonkie)\n",
        "- **PyPI**: [pypi.org/project/chonkie](https://pypi.org/project/chonkie)\n",
        "\n",
        "### Recursos Adicionais\n",
        "- **RAG Best Practices**: [Retrieval-Augmented Generation Guide](https://example.com/rag-guide)\n",
        "- **Embedding Models**: [Sentence Transformers](https://www.sbert.net/)\n",
        "- **Tokenization**: [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/)\n",
        "\n",
        "---\n",
        "\n",
        "Jo√£o Gabriel Lima  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
