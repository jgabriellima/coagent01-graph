{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Docling - Advanced Document Processing\n",
        "\n",
        "`by JoÃ£o Gabriel Lima`\n",
        "\n",
        "## Description\n",
        "Docling Ã© um toolkit open-source da IBM Research para conversÃ£o avanÃ§ada de documentos em formatos estruturados para IA Generativa. Desenvolvido especificamente para parsing de PDFs complexos, DOCX, XLSX, HTML e imagens com compreensÃ£o de layout, tabelas, cÃ³digo, fÃ³rmulas e classificaÃ§Ã£o de imagens. IntegraÃ§Ã£o essencial para extraÃ§Ã£o de conteÃºdo estruturado em projetos de AI.\n",
        "\n",
        "## Prerequisites\n",
        "- **Precida de API Key:** NÃ£o - biblioteca open source\n",
        "- **Dependencies:** docling, docling-core, transformers\n",
        "- **Data Input:** data/ directory, formatos PDF/DOCX/XLSX/HTML/images\n",
        "- **Last Update:** v2.38.0 (Jun 23, 2025) - 32.7k GitHub stars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation\n",
        "!uv add docling docling-core transformers torch pillow pandas tabulate\n",
        "!uv add \"docling[complete]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from docling.document_converter import DocumentConverter\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "from docling_core.types.doc import DoclingDocument\n",
        "from docling_core.types.doc.document import DocTagsDocument\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# ConfiguraÃ§Ã£o de diretÃ³rios\n",
        "DATA_INPUT = Path(\"../../data/input\")\n",
        "DATA_OUTPUT = Path(\"../../data/output\")\n",
        "DATA_PROCESSED = Path(\"../../data/processed\")\n",
        "\n",
        "# Criar diretÃ³rios se nÃ£o existirem\n",
        "for dir_path in [DATA_INPUT, DATA_OUTPUT, DATA_PROCESSED]:\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"âœ… Docling environment configured successfully!\")\n",
        "print(f\"ðŸ“ Input directory: {DATA_INPUT.resolve()}\")\n",
        "print(f\"ðŸ“ Output directory: {DATA_OUTPUT.resolve()}\")\n",
        "print(f\"ðŸ“ Processed directory: {DATA_PROCESSED.resolve()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Scenarios\n",
        "\n",
        "### 1. Basic Document Conversion\n",
        "\n",
        "ConversÃ£o bÃ¡sica de documentos usando configuraÃ§Ãµes padrÃ£o do Docling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_basic_document(source_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    ConversÃ£o bÃ¡sica de documento usando Docling\n",
        "    \n",
        "    Args:\n",
        "        source_path: Caminho para o documento a ser convertido\n",
        "        \n",
        "    Returns:\n",
        "        Dict com formatos de saÃ­da (markdown, json, html) e documento original\n",
        "    \"\"\"\n",
        "    converter = DocumentConverter()\n",
        "    result = converter.convert(source_path)\n",
        "    \n",
        "    return {\n",
        "        'markdown': result.document.export_to_markdown(),\n",
        "        'json': result.document.export_to_json(),\n",
        "        'html': result.document.export_to_html(),\n",
        "        'document': result.document\n",
        "    }\n",
        "\n",
        "print(\"âœ… Basic document conversion function ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2. Advanced Document Processing\n",
        "\n",
        "Processamento avanÃ§ado com configuraÃ§Ãµes otimizadas para extraÃ§Ã£o de estruturas complexas como tabelas, imagens e layouts especÃ­ficos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_document_advanced(file_path: str, enable_ocr: bool = True) -> dict:\n",
        "    \"\"\"\n",
        "    Parse avanÃ§ado de documento com configuraÃ§Ãµes otimizadas\n",
        "    \n",
        "    Args:\n",
        "        file_path: Caminho para o arquivo\n",
        "        enable_ocr: Se deve ativar OCR para documentos escaneados\n",
        "        \n",
        "    Returns:\n",
        "        Dict com conteÃºdo estruturado e metadados\n",
        "    \"\"\"\n",
        "    # ConfiguraÃ§Ã£o otimizada para documentos complexos\n",
        "    pipeline_options = PdfPipelineOptions()\n",
        "    pipeline_options.do_ocr = enable_ocr\n",
        "    pipeline_options.do_table_structure = True\n",
        "    pipeline_options.table_structure_options.do_cell_matching = True\n",
        "    pipeline_options.do_picture_extraction = True\n",
        "    \n",
        "    converter = DocumentConverter(\n",
        "        format_options={\n",
        "            InputFormat.PDF: pipeline_options,\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    result = converter.convert(file_path)\n",
        "    \n",
        "    # Extrair estrutura de seÃ§Ãµes do documento\n",
        "    structured_content = extract_document_structure(result.document)\n",
        "    \n",
        "    return {\n",
        "        'success': True,\n",
        "        'metadata': extract_document_metadata(result.document),\n",
        "        'content': structured_content,\n",
        "        'raw_markdown': result.document.export_to_markdown(),\n",
        "        'raw_json': result.document.export_to_json()\n",
        "    }\n",
        "\n",
        "print(\"âœ… Advanced document processing function ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Document Structure Extraction\n",
        "\n",
        "FunÃ§Ãµes para extrair estruturas hierÃ¡rquicas de documentos acadÃªmicos e tÃ©cnicos, identificando seÃ§Ãµes comuns como abstract, introduÃ§Ã£o, metodologia, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_document_structure(document) -> dict:\n",
        "    \"\"\"\n",
        "    Extrair estrutura hierÃ¡rquica de documentos acadÃªmicos/tÃ©cnicos\n",
        "    \n",
        "    Args:\n",
        "        document: DoclingDocument object\n",
        "        \n",
        "    Returns:\n",
        "        Dict com seÃ§Ãµes estruturadas do documento\n",
        "    \"\"\"\n",
        "    markdown_content = document.export_to_markdown()\n",
        "    \n",
        "    sections = {\n",
        "        'title': '',\n",
        "        'authors': '',\n",
        "        'abstract': '',\n",
        "        'introduction': '',\n",
        "        'methodology': '',\n",
        "        'results': '',\n",
        "        'discussion': '',\n",
        "        'conclusion': '',\n",
        "        'references': '',\n",
        "        'appendix': ''\n",
        "    }\n",
        "    \n",
        "    lines = markdown_content.split('\\\\n')\n",
        "    current_section = None\n",
        "    \n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        \n",
        "        if line.startswith('# '):\n",
        "            sections['title'] = line[2:]\n",
        "        elif any(keyword in line.lower() for keyword in ['## abstract', '## resumo']):\n",
        "            current_section = 'abstract'\n",
        "        elif any(keyword in line.lower() for keyword in ['## introduction', '## introduÃ§Ã£o']):\n",
        "            current_section = 'introduction'\n",
        "        elif any(keyword in line.lower() for keyword in ['## methods', '## methodology', '## metodologia']):\n",
        "            current_section = 'methodology'\n",
        "        elif any(keyword in line.lower() for keyword in ['## results', '## resultados']):\n",
        "            current_section = 'results'\n",
        "        elif any(keyword in line.lower() for keyword in ['## discussion', '## discussÃ£o']):\n",
        "            current_section = 'discussion'\n",
        "        elif any(keyword in line.lower() for keyword in ['## conclusion', '## conclusÃ£o']):\n",
        "            current_section = 'conclusion'\n",
        "        elif any(keyword in line.lower() for keyword in ['## references', '## referÃªncias']):\n",
        "            current_section = 'references'\n",
        "        elif any(keyword in line.lower() for keyword in ['## appendix', '## apÃªndice']):\n",
        "            current_section = 'appendix'\n",
        "        elif current_section and line:\n",
        "            sections[current_section] += line + '\\\\n'\n",
        "    \n",
        "    return sections\n",
        "\n",
        "def extract_document_metadata(document) -> dict:\n",
        "    \"\"\"\n",
        "    Extrair metadados abrangentes do documento\n",
        "    \n",
        "    Args:\n",
        "        document: DoclingDocument object\n",
        "        \n",
        "    Returns:\n",
        "        Dict com metadados estruturados\n",
        "    \"\"\"\n",
        "    markdown = document.export_to_markdown()\n",
        "    \n",
        "    lines = markdown.split('\\\\n')\n",
        "    title = \"\"\n",
        "    for line in lines:\n",
        "        if line.startswith('# '):\n",
        "            title = line[2:].strip()\n",
        "            break\n",
        "    \n",
        "    return {\n",
        "        'title': title,\n",
        "        'word_count': len(markdown.split()),\n",
        "        'has_tables': '|' in markdown,\n",
        "        'has_images': '![' in markdown,\n",
        "        'sections': count_document_sections(markdown),\n",
        "        'language': detect_document_language(markdown)\n",
        "    }\n",
        "\n",
        "def count_document_sections(markdown: str) -> dict:\n",
        "    \"\"\"\n",
        "    Contar seÃ§Ãµes tÃ­picas de documentos acadÃªmicos/tÃ©cnicos\n",
        "    \"\"\"\n",
        "    sections = ['abstract', 'introduction', 'methodology', 'results', 'discussion', 'conclusion']\n",
        "    counts = {}\n",
        "    \n",
        "    for section in sections:\n",
        "        counts[section] = markdown.lower().count(f'## {section}')\n",
        "    \n",
        "    return counts\n",
        "\n",
        "def detect_document_language(markdown: str) -> str:\n",
        "    \"\"\"\n",
        "    Detectar idioma predominante do documento\n",
        "    \"\"\"\n",
        "    # Palavras-chave em portuguÃªs\n",
        "    pt_keywords = ['resumo', 'introduÃ§Ã£o', 'metodologia', 'resultados', 'discussÃ£o', 'conclusÃ£o']\n",
        "    # Palavras-chave em inglÃªs\n",
        "    en_keywords = ['abstract', 'introduction', 'methodology', 'results', 'discussion', 'conclusion']\n",
        "    \n",
        "    pt_count = sum(1 for keyword in pt_keywords if keyword in markdown.lower())\n",
        "    en_count = sum(1 for keyword in en_keywords if keyword in markdown.lower())\n",
        "    \n",
        "    return 'portuguese' if pt_count > en_count else 'english'\n",
        "\n",
        "print(\"âœ… Document structure extraction functions ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3. SmolDocling VLM Integration\n",
        "\n",
        "IntegraÃ§Ã£o com o modelo SmolDocling para processamento visual de documentos usando Vision Language Models (VLM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SmolDocling VLM - Ultra-compact 256M parameter model\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "from transformers.image_utils import load_image\n",
        "\n",
        "def setup_smoldocling():\n",
        "    \"\"\"\n",
        "    Setup SmolDocling VLM model for advanced document understanding\n",
        "    \n",
        "    Returns:\n",
        "        Tuple com (processor, model, device) configurados\n",
        "    \"\"\"\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    \n",
        "    # Initialize processor and model\n",
        "    processor = AutoProcessor.from_pretrained(\"ds4sd/SmolDocling-256M-preview\")\n",
        "    model = AutoModelForVision2Seq.from_pretrained(\n",
        "        \"ds4sd/SmolDocling-256M-preview\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\",\n",
        "    ).to(DEVICE)\n",
        "    \n",
        "    return processor, model, DEVICE\n",
        "\n",
        "print(\"âœ… SmolDocling setup function ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. SmolDocling VLM Integration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Advanced Features & Capabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch Processing with Performance Optimization\n",
        "def batch_process_documents_optimized(input_dir: str, output_dir: str, use_vlm: bool = False) -> list:\n",
        "    \"\"\"\n",
        "    Processamento em lote otimizado com mÃºltiplas configuraÃ§Ãµes\n",
        "    \"\"\"\n",
        "    input_path = Path(input_dir)\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    # ConfiguraÃ§Ã£o para diferentes tipos de documento\n",
        "    configs = {\n",
        "        'medical': {\n",
        "            'do_ocr': True,\n",
        "            'do_table_structure': True,\n",
        "            'do_picture_extraction': True,\n",
        "            'table_structure_options.do_cell_matching': True\n",
        "        },\n",
        "        'presentation': {\n",
        "            'do_ocr': False,\n",
        "            'do_table_structure': True,\n",
        "            'do_picture_extraction': False,\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    for file_path in input_path.glob(\"*\"):\n",
        "        if file_path.suffix.lower() in ['.pdf', '.docx', '.xlsx', '.html', '.png', '.jpg', '.jpeg']:\n",
        "            print(f\"ðŸ”„ Processing: {file_path.name}\")\n",
        "            \n",
        "            try:\n",
        "                if use_vlm and file_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
        "                    # Use SmolDocling for images\n",
        "                    result = convert_with_smoldocling(str(file_path))\n",
        "                    content = result['markdown']\n",
        "                else:\n",
        "                    # Use standard Docling\n",
        "                    result = convert_basic_document(str(file_path))\n",
        "                    content = result['markdown']\n",
        "                \n",
        "                # Save result\n",
        "                output_file = output_path / f\"{file_path.stem}_parsed.md\"\n",
        "                with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                    f.write(content)\n",
        "                \n",
        "                results.append({\n",
        "                    \"file\": file_path.name,\n",
        "                    \"status\": \"success\",\n",
        "                    \"method\": \"vlm\" if use_vlm else \"standard\",\n",
        "                    \"output\": str(output_file),\n",
        "                    \"size_kb\": file_path.stat().st_size // 1024\n",
        "                })\n",
        "                \n",
        "            except Exception as e:\n",
        "                results.append({\n",
        "                    \"file\": file_path.name,\n",
        "                    \"status\": \"error\",\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Pipeline Integration Class\n",
        "class DoclingPipeline:\n",
        "    \"\"\"\n",
        "    Pipeline integrado para DocuMed.ai com fallbacks e otimizaÃ§Ãµes\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, use_vlm=False, medical_optimized=True):\n",
        "        self.use_vlm = use_vlm\n",
        "        self.medical_optimized = medical_optimized\n",
        "        self.setup_converter()\n",
        "        \n",
        "        if use_vlm:\n",
        "            self.processor, self.model, self.device = setup_smoldocling()\n",
        "    \n",
        "    def setup_converter(self):\n",
        "        \"\"\"Setup converter with medical optimizations\"\"\"\n",
        "        if self.medical_optimized:\n",
        "            pipeline_options = PdfPipelineOptions()\n",
        "            pipeline_options.do_ocr = True\n",
        "            pipeline_options.do_table_structure = True\n",
        "            pipeline_options.table_structure_options.do_cell_matching = True\n",
        "            pipeline_options.do_picture_extraction = True\n",
        "            \n",
        "            self.converter = DocumentConverter(\n",
        "                format_options={\n",
        "                    InputFormat.PDF: pipeline_options,\n",
        "                }\n",
        "            )\n",
        "        else:\n",
        "            self.converter = DocumentConverter()\n",
        "    \n",
        "    def process_document(self, source_path: str) -> dict:\n",
        "        \"\"\"\n",
        "        Processo principal com fallback automÃ¡tico\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Try VLM first if enabled\n",
        "            if self.use_vlm and Path(source_path).suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
        "                return self._process_with_vlm(source_path)\n",
        "            else:\n",
        "                return self._process_with_standard(source_path)\n",
        "                \n",
        "        except Exception as e:\n",
        "            # Fallback to basic processing\n",
        "            print(f\"âš ï¸ Fallback to basic processing: {str(e)}\")\n",
        "            return self._process_basic_fallback(source_path)\n",
        "    \n",
        "    def _process_with_vlm(self, source_path: str) -> dict:\n",
        "        \"\"\"Process with SmolDocling VLM\"\"\"\n",
        "        return convert_with_smoldocling(source_path)\n",
        "    \n",
        "    def _process_with_standard(self, source_path: str) -> dict:\n",
        "        \"\"\"Process with standard Docling\"\"\"\n",
        "        result = self.converter.convert(source_path)\n",
        "        \n",
        "        return {\n",
        "            'success': True,\n",
        "            'document': result.document,\n",
        "            'markdown': result.document.export_to_markdown(),\n",
        "            'json': result.document.export_to_json(),\n",
        "            'metadata': self._extract_metadata(result.document)\n",
        "        }\n",
        "    \n",
        "    def _process_basic_fallback(self, source_path: str) -> dict:\n",
        "        \"\"\"Basic fallback processing\"\"\"\n",
        "        converter = DocumentConverter()\n",
        "        result = converter.convert(source_path)\n",
        "        \n",
        "        return {\n",
        "            'success': True,\n",
        "            'fallback': True,\n",
        "            'document': result.document,\n",
        "            'markdown': result.document.export_to_markdown(),\n",
        "            'json': result.document.export_to_json()\n",
        "        }\n",
        "    \n",
        "    def _extract_metadata(self, document) -> dict:\n",
        "        \"\"\"Extract comprehensive metadata\"\"\"\n",
        "        markdown = document.export_to_markdown()\n",
        "        \n",
        "        return {\n",
        "            'title': self._extract_title(markdown),\n",
        "            'word_count': len(markdown.split()),\n",
        "            'has_tables': len(document.tables) > 0,\n",
        "            'has_images': len(document.pictures) > 0,\n",
        "            'table_count': len(document.tables),\n",
        "            'image_count': len(document.pictures),\n",
        "            'sections': self._count_sections(markdown),\n",
        "            'confidence': self._estimate_confidence(document)\n",
        "        }\n",
        "    \n",
        "    def _extract_title(self, markdown: str) -> str:\n",
        "        \"\"\"Extract document title\"\"\"\n",
        "        lines = markdown.split('\\\\n')\n",
        "        for line in lines:\n",
        "            if line.startswith('# '):\n",
        "                return line[2:].strip()\n",
        "        return \"\"\n",
        "    \n",
        "    def _count_sections(self, markdown: str) -> dict:\n",
        "        \"\"\"Count document sections\"\"\"\n",
        "        sections = ['abstract', 'introduction', 'methods', 'results', 'discussion', 'conclusion']\n",
        "        counts = {}\n",
        "        for section in sections:\n",
        "            counts[section] = markdown.lower().count(f'## {section}')\n",
        "        return counts\n",
        "    \n",
        "    def _estimate_confidence(self, document) -> float:\n",
        "        \"\"\"Estimate processing confidence based on document features\"\"\"\n",
        "        confidence = 0.5  # Base confidence\n",
        "        \n",
        "        # Increase confidence for structured elements\n",
        "        if len(document.tables) > 0:\n",
        "            confidence += 0.2\n",
        "        if len(document.pictures) > 0:\n",
        "            confidence += 0.1\n",
        "        \n",
        "        # Decrease confidence for potential issues\n",
        "        markdown = document.export_to_markdown()\n",
        "        if len(markdown) < 100:\n",
        "            confidence -= 0.2\n",
        "        \n",
        "        return min(max(confidence, 0.0), 1.0)\n",
        "\n",
        "print(\"âœ… Advanced pipeline and batch processing ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance Optimization and Monitoring\n",
        "def monitor_parsing_performance(results: list) -> dict:\n",
        "    \"\"\"\n",
        "    Monitor and analyze parsing performance metrics\n",
        "    \"\"\"\n",
        "    successful = [r for r in results if r.get('status') == 'success']\n",
        "    failed = [r for r in results if r.get('status') == 'error']\n",
        "    \n",
        "    metrics = {\n",
        "        'total_documents': len(results),\n",
        "        'successful_parses': len(successful),\n",
        "        'failed_parses': len(failed),\n",
        "        'success_rate': len(successful) / len(results) if results else 0,\n",
        "        'avg_file_size_kb': sum(r.get('size_kb', 0) for r in successful) / len(successful) if successful else 0,\n",
        "        'methods_used': {\n",
        "            'standard': len([r for r in successful if r.get('method') == 'standard']),\n",
        "            'vlm': len([r for r in successful if r.get('method') == 'vlm'])\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Error Handling and Recovery\n",
        "def robust_document_processing(file_path: str, max_retries: int = 3) -> dict:\n",
        "    \"\"\"\n",
        "    Processamento robusto com mÃºltiplas tentativas e fallbacks\n",
        "    \"\"\"\n",
        "    strategies = [\n",
        "        lambda: DoclingPipeline(use_vlm=True, medical_optimized=True).process_document(file_path),\n",
        "        lambda: DoclingPipeline(use_vlm=False, medical_optimized=True).process_document(file_path),\n",
        "        lambda: DoclingPipeline(use_vlm=False, medical_optimized=False).process_document(file_path),\n",
        "    ]\n",
        "    \n",
        "    for attempt, strategy in enumerate(strategies):\n",
        "        try:\n",
        "            result = strategy()\n",
        "            result['strategy_used'] = attempt + 1\n",
        "            result['attempts'] = attempt + 1\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            if attempt == len(strategies) - 1:\n",
        "                return {\n",
        "                    'success': False,\n",
        "                    'error': str(e),\n",
        "                    'attempts': len(strategies),\n",
        "                    'file': file_path\n",
        "                }\n",
        "            continue\n",
        "\n",
        "# Integration with DocuMed.ai workflow\n",
        "def integrate_with_documedai_pipeline(source_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    IntegraÃ§Ã£o completa com pipeline DocuMed.ai\n",
        "    \"\"\"\n",
        "    # Initialize pipeline\n",
        "    pipeline = DoclingPipeline(use_vlm=True, medical_optimized=True)\n",
        "    \n",
        "    # Process document\n",
        "    result = pipeline.process_document(source_path)\n",
        "    \n",
        "    if result.get('success'):\n",
        "        # Extract medical-specific structure\n",
        "        if 'document' in result:\n",
        "            medical_structure = extract_medical_structure(result['document'])\n",
        "            result['medical_structure'] = medical_structure\n",
        "        \n",
        "        # Save to DocuMed.ai format\n",
        "        output_path = DATA_PROCESSED / f\"{Path(source_path).stem}_documedai.json\"\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump({\n",
        "                'source': source_path,\n",
        "                'processed_at': pd.Timestamp.now().isoformat(),\n",
        "                'metadata': result.get('metadata', {}),\n",
        "                'medical_structure': result.get('medical_structure', {}),\n",
        "                'markdown': result.get('markdown', ''),\n",
        "                'confidence': result.get('metadata', {}).get('confidence', 0.5)\n",
        "            }, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        result['documedai_output'] = str(output_path)\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"âœ… Performance monitoring and robust processing ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Edge Cases & Limitations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Known Issues and Limitations\n",
        "- **Large Files**: Performance degradation with PDFs >100MB - consider splitting documents\n",
        "- **OCR Language Support**: Best performance with Portuguese/English text\n",
        "- **Complex Layouts**: Some scientific layouts with overlapping elements may not parse correctly\n",
        "- **Memory Requirements**: High RAM usage for large documents (recommend 8GB+ for batch processing)\n",
        "- **SmolDocling Flash Attention**: T4 GPUs require `_attn_implementation=\\\"eager\\\"` instead of flash_attention_2\n",
        "- **Rate Limiting**: No built-in rate limiting for batch processing - implement delays if needed\n",
        "\n",
        "### Special Considerations\n",
        "- **Scanned PDFs**: Enable OCR for best results with image-based documents\n",
        "- **Tables with Merged Cells**: Complex table structures may require manual verification\n",
        "- **Medical Images**: SmolDocling VLM performs better on charts/diagrams than raw medical imaging\n",
        "- **Multi-language Documents**: Mixed language documents may have inconsistent extraction quality\n",
        "- **Handwritten Content**: Limited support for handwritten text in medical documents\n",
        "\n",
        "### Troubleshooting Guide\n",
        "1. **Memory Errors**: Reduce batch size or process documents individually\n",
        "2. **OCR Failures**: Check document image quality and language settings\n",
        "3. **Table Extraction Issues**: Verify table structure and consider manual preprocessing\n",
        "4. **VLM Errors**: Fallback to standard Docling processing\n",
        "5. **Encoding Issues**: Ensure UTF-8 encoding for output files\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## References & Last Update\n",
        "\n",
        "### Official Documentation\n",
        "- **Official Docs**: [docling-project.github.io/docling](https://docling-project.github.io/docling/)\n",
        "- **Repository**: [github.com/docling-project/docling](https://github.com/docling-project/docling)\n",
        "- **SmolDocling HuggingFace**: [ds4sd/SmolDocling-256M-preview](https://huggingface.co/ds4sd/SmolDocling-256M-preview)\n",
        "- **Technical Report**: [arXiv:2408.09869](https://arxiv.org/abs/2408.09869)\n",
        "- **SmolDocling Paper**: [arXiv:2503.11576](https://arxiv.org/abs/2503.11576)\n",
        "- **IBM Research**: [research.ibm.com/publications/docling](https://research.ibm.com/publications/docling-technical-report)\n",
        "\n",
        "### Integration Examples\n",
        "- **LangChain Integration**: [Official Examples](https://docling-project.github.io/docling/examples/rag_with_langchain/)\n",
        "- **LlamaIndex Integration**: [Official Examples](https://docling-project.github.io/docling/examples/rag_with_llamaindex/)\n",
        "- **Haystack Integration**: [Official Examples](https://docling-project.github.io/docling/examples/rag_with_haystack/)\n",
        "\n",
        "### Version Information\n",
        "- **Last Updated**: June 16, 2025 (v2.37.0)\n",
        "- **SmolDocling Release**: March 14, 2025\n",
        "- **GitHub Stars**: 32.4k+ (as of June 2025)\n",
        "- **License**: MIT License\n",
        "- **Python Support**: 3.9+\n",
        "- **Platform Support**: macOS, Linux, Windows (x86_64, arm64)\"\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
