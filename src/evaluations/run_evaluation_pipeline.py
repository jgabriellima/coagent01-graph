#!/usr/bin/env python3
"""
Pipeline Completo de Evaluation
------------------------------

Este script executa o pipeline completo de evaluation:
1. Gera dados sintÃ©ticos baseados no sistema de swarm agents
2. Executa cenÃ¡rios nos agentes para coletar traces via LangSmith
3. Executa suite de evaluators nos traces coletados
4. Gera relatÃ³rios consolidados

Uso:
    python run_evaluation_pipeline.py [OPTIONS]

Exemplos:
    # Pipeline completo
    python run_evaluation_pipeline.py --project "my-evaluation" --scenarios 20
    
    # Pipeline rÃ¡pido para testes
    python run_evaluation_pipeline.py --quick-test
    
    # Apenas geraÃ§Ã£o de dados sintÃ©ticos
    python run_evaluation_pipeline.py --only-generate
    
    # Apenas evaluation (usar dados existentes)
    python run_evaluation_pipeline.py --only-evaluate --project "existing-project"
"""

import argparse
import asyncio
import os
import sys
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# Adicionar o diretÃ³rio raiz do projeto ao sys.path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.evaluations.synthetic_data_generator import SyntheticDataGenerator
from src.evaluations.evaluators.run_evaluations import EvaluationRunner
from src.evaluations.evaluators.evaluator_registry import get_evaluators_for_profile
from langsmith import Client


def parse_args():
    """Processa argumentos da linha de comando"""
    parser = argparse.ArgumentParser(
        description="Pipeline Completo de Evaluation para Sistema de Swarm Agents",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  %(prog)s --project "my-evaluation" --scenarios 20
  %(prog)s --quick-test
  %(prog)s --only-generate --scenarios 10
  %(prog)s --only-evaluate --project "existing-project"
        """
    )
    
    # ConfiguraÃ§Ãµes gerais
    parser.add_argument(
        "--project",
        type=str,
        help="Nome do projeto LangSmith (padrÃ£o: evaluation-pipeline-TIMESTAMP)"
    )
    
    parser.add_argument(
        "--scenarios",
        type=int,
        default=15,
        help="NÃºmero de cenÃ¡rios por agente (padrÃ£o: 15)"
    )
    
    parser.add_argument(
        "--model",
        type=str,
        default="openai:gpt-4o-mini",
        help="Modelo a usar (padrÃ£o: openai:gpt-4o-mini)"
    )
    
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="Temperatura do modelo (padrÃ£o: 0.7)"
    )
    
    parser.add_argument(
        "--include-swarm",
        action="store_true",
        default=True,
        help="Incluir cenÃ¡rios de swarm workflow (padrÃ£o: True)"
    )
    
    parser.add_argument(
        "--no-swarm",
        action="store_true",
        help="Excluir cenÃ¡rios de swarm workflow"
    )
    
    parser.add_argument(
        "--generation-mode",
        type=str,
        choices=["static", "llm", "hybrid"],
        default="hybrid",
        help="Modo de geraÃ§Ã£o de cenÃ¡rios (padrÃ£o: hybrid)"
    )
    
    # Modo de execuÃ§Ã£o
    parser.add_argument(
        "--only-generate",
        action="store_true",
        help="Apenas gerar dados sintÃ©ticos, nÃ£o executar evaluation"
    )
    
    parser.add_argument(
        "--only-evaluate",
        action="store_true",
        help="Apenas executar evaluation em dados existentes"
    )
    
    parser.add_argument(
        "--quick-test",
        action="store_true",
        help="ExecuÃ§Ã£o rÃ¡pida com poucos cenÃ¡rios para teste"
    )
    
    # ConfiguraÃ§Ãµes de evaluation
    parser.add_argument(
        "--evaluator-profile",
        type=str,
        default="agentic",
        choices=["agentic", "comprehensive", "minimal"],
        help="Perfil de evaluators a usar (padrÃ£o: agentic)"
    )
    
    parser.add_argument(
        "--evaluation-model",
        type=str,
        default="openai:gpt-4o",
        help="Modelo para evaluation (padrÃ£o: openai:gpt-4o)"
    )
    
    # ConfiguraÃ§Ãµes de saÃ­da
    parser.add_argument(
        "--output-dir",
        type=str,
        default="sample_agent/evaluations/results",
        help="DiretÃ³rio para salvar resultados (padrÃ£o: sample_agent/evaluations/results)"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="SaÃ­da detalhada durante a execuÃ§Ã£o"
    )
    
    return parser.parse_args()


class EvaluationPipeline:
    """Pipeline completo de evaluation"""
    
    def __init__(self, args):
        self.args = args
        self.project_name = args.project or f"evaluation-pipeline-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Configurar LangSmith client
        self.langsmith_client = None
        if os.getenv("LANGSMITH_API_KEY"):
            try:
                self.langsmith_client = Client()
            except Exception as e:
                print(f"âš ï¸  Erro ao configurar LangSmith: {e}")
    
    async def run_data_generation(self) -> Dict[str, Any]:
        """Executa geraÃ§Ã£o de dados sintÃ©ticos"""
        print("ðŸŽ² FASE 1: GeraÃ§Ã£o de Dados SintÃ©ticos")
        print("="*50)
        
        # Resolver configuraÃ§Ã£o do swarm
        include_swarm = self.args.include_swarm and not self.args.no_swarm
        
        # Configurar gerador
        generator = SyntheticDataGenerator(
            model_name=self.args.model,
            temperature=self.args.temperature,
            langsmith_project_name=self.project_name,
            num_scenarios_per_agent=self.args.scenarios,
            include_swarm_scenarios=include_swarm,
            scenario_generation_mode=self.args.generation_mode
        )
        
        # Gerar e executar cenÃ¡rios
        scenarios = await generator.generate_all_scenarios()
        results = await generator.execute_all_scenarios(scenarios)
        
        # Gerar relatÃ³rio
        report = generator.generate_report(results)
        
        # Salvar relatÃ³rio
        report_path = self.output_dir / f"synthetic_data_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Dados sintÃ©ticos gerados e executados")
        print(f"ðŸ“„ RelatÃ³rio salvo em: {report_path}")
        print(f"ðŸ“Š Projeto LangSmith: {self.project_name}")
        
        return report
    
    async def run_evaluation(self) -> Dict[str, Any]:
        """Executa suite de evaluators"""
        print(f"\nðŸ” FASE 2: Execution de Evaluators")
        print("="*50)
        
        if not self.langsmith_client:
            print("âŒ LangSmith client nÃ£o disponÃ­vel. NÃ£o Ã© possÃ­vel executar evaluators.")
            return {}
        
        try:
            # Obter runs do projeto
            runs = list(self.langsmith_client.list_runs(project_name=self.project_name))
            
            if not runs:
                print(f"âš ï¸  Nenhum run encontrado no projeto: {self.project_name}")
                return {}
            
            print(f"ðŸ“Š Encontrados {len(runs)} runs para evaluation")
            
            # Obter evaluators para o perfil especificado
            evaluators_list = get_evaluators_for_profile(self.args.evaluator_profile)
            evaluators = {evaluator.name(): evaluator for evaluator in evaluators_list}
            
            print(f"ðŸ”§ Usando perfil '{self.args.evaluator_profile}' com {len(evaluators)} evaluators:")
            for name in evaluators.keys():
                print(f"   - {name}")
            print()
            
            # Configurar runner
            runner = EvaluationRunner(
                evaluators=evaluators,
                langsmith_client=self.langsmith_client,
                evaluation_model=self.args.evaluation_model
            )
            
            # Executar evaluations
            results = await runner.run_evaluations(
                project_name=self.project_name,
                max_runs=None,  # Avaliar todos os runs
                batch_size=10
            )
            
            # Salvar resultados
            results_path = self.output_dir / f"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(results_path, "w", encoding="utf-8") as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… Evaluation concluÃ­da")
            print(f"ðŸ“„ Resultados salvos em: {results_path}")
            
            return results
            
        except Exception as e:
            print(f"âŒ Erro durante evaluation: {e}")
            if self.args.verbose:
                import traceback
                traceback.print_exc()
            return {}
    
    def generate_consolidated_report(self, data_report: Dict[str, Any], eval_results: Dict[str, Any]) -> Dict[str, Any]:
        """Gera relatÃ³rio consolidado"""
        print(f"\nðŸ“Š FASE 3: GeraÃ§Ã£o de RelatÃ³rio Consolidado")
        print("="*50)
        
        # Consolidar mÃ©tricas
        consolidated = {
            "pipeline_summary": {
                "project_name": self.project_name,
                "executed_at": datetime.now().isoformat(),
                "configuration": {
                    "model": self.args.model,
                    "temperature": self.args.temperature,
                    "scenarios_per_agent": self.args.scenarios,
                    "evaluator_profile": self.args.evaluator_profile,
                    "evaluation_model": self.args.evaluation_model
                }
            },
            "data_generation": data_report.get("summary", {}),
            "evaluation_results": eval_results.get("summary", {}),
            "detailed_results": {
                "data_generation": data_report,
                "evaluation": eval_results
            }
        }
        
        # Calcular mÃ©tricas combinadas
        if data_report and eval_results:
            # Taxa de sucesso geral
            data_success_rate = data_report.get("summary", {}).get("success_rate", 0)
            eval_success_rate = eval_results.get("summary", {}).get("success_rate", 0)
            
            consolidated["pipeline_summary"]["overall_success_rate"] = (data_success_rate + eval_success_rate) / 2
            
            # Tempo total
            data_time = data_report.get("summary", {}).get("avg_execution_time", 0)
            eval_time = eval_results.get("summary", {}).get("avg_evaluation_time", 0)
            
            consolidated["pipeline_summary"]["total_avg_time"] = data_time + eval_time
        
        # Salvar relatÃ³rio consolidado
        report_path = self.output_dir / f"consolidated_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(consolidated, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… RelatÃ³rio consolidado gerado")
        print(f"ðŸ“„ RelatÃ³rio salvo em: {report_path}")
        
        return consolidated
    
    def print_summary(self, consolidated_report: Dict[str, Any]):
        """Imprime resumo final"""
        print("\n" + "="*70)
        print("ðŸ“Š RESUMO FINAL DO PIPELINE")
        print("="*70)
        
        summary = consolidated_report.get("pipeline_summary", {})
        data_summary = consolidated_report.get("data_generation", {})
        eval_summary = consolidated_report.get("evaluation_results", {})
        
        print(f"ðŸŽ¯ Projeto: {summary.get('project_name', 'N/A')}")
        print(f"âš™ï¸  Modelo: {summary.get('configuration', {}).get('model', 'N/A')}")
        print(f"ðŸ“ˆ Perfil de Evaluation: {summary.get('configuration', {}).get('evaluator_profile', 'N/A')}")
        
        if data_summary:
            print(f"\nðŸ“Š GeraÃ§Ã£o de Dados:")
            print(f"   CenÃ¡rios: {data_summary.get('total_scenarios', 0)}")
            print(f"   Sucessos: {data_summary.get('successful', 0)}")
            print(f"   Taxa de sucesso: {data_summary.get('success_rate', 0):.1f}%")
            print(f"   Tempo mÃ©dio: {data_summary.get('avg_execution_time', 0):.2f}s")
        
        if eval_summary:
            print(f"\nðŸ” Evaluation:")
            print(f"   Runs avaliados: {eval_summary.get('total_runs', 0)}")
            print(f"   Evaluators executados: {eval_summary.get('evaluators_executed', 0)}")
            print(f"   Taxa de sucesso: {eval_summary.get('success_rate', 0):.1f}%")
        
        overall_success = summary.get("overall_success_rate", 0)
        if overall_success > 0:
            print(f"\nðŸŽ¯ Taxa de Sucesso Geral: {overall_success:.1f}%")
        
        print(f"\nðŸŽ¯ PRÃ“XIMOS PASSOS:")
        print(f"1. Analise os resultados detalhados nos arquivos JSON")
        print(f"2. Verifique traces e mÃ©tricas no LangSmith dashboard")
        print(f"3. Ajuste configuraÃ§Ãµes baseado nos resultados")
        print(f"4. Execute novamente com configuraÃ§Ãµes otimizadas")
        
        if overall_success < 80:
            print(f"\nâš ï¸  Taxa de sucesso baixa ({overall_success:.1f}%)")
            print(f"   Considere revisar configuraÃ§Ãµes dos agentes ou evaluators")
    
    async def run_pipeline(self):
        """Executa pipeline completo"""
        print("ðŸš€ INICIANDO PIPELINE DE EVALUATION")
        print("="*70)
        print(f"Projeto: {self.project_name}")
        print(f"CenÃ¡rios por agente: {self.args.scenarios}")
        print(f"Perfil de evaluators: {self.args.evaluator_profile}")
        print(f"DiretÃ³rio de saÃ­da: {self.output_dir}")
        print(f"Incluir swarm: {self.args.include_swarm and not self.args.no_swarm}")
        print(f"Modo de geraÃ§Ã£o: {self.args.generation_mode}")
        print()
        
        data_report = {}
        eval_results = {}
        
        try:
            # Fase 1: GeraÃ§Ã£o de dados sintÃ©ticos
            if not self.args.only_evaluate:
                data_report = await self.run_data_generation()
            
            # Fase 2: Execution de evaluators
            if not self.args.only_generate:
                eval_results = await self.run_evaluation()
            
            # Fase 3: RelatÃ³rio consolidado
            consolidated_report = self.generate_consolidated_report(data_report, eval_results)
            
            # Resumo final
            self.print_summary(consolidated_report)
            
        except Exception as e:
            print(f"âŒ Erro no pipeline: {e}")
            if self.args.verbose:
                import traceback
                traceback.print_exc()
            sys.exit(1)


async def main():
    """FunÃ§Ã£o principal"""
    args = parse_args()
    
    # ConfiguraÃ§Ãµes para teste rÃ¡pido
    if args.quick_test:
        args.scenarios = 3
        args.evaluator_profile = "minimal"
        args.project = args.project or "quick-test-pipeline"
        print("ðŸš€ Modo de teste rÃ¡pido ativado!")
    
    # Verificar variÃ¡veis de ambiente
    if not os.getenv("LANGSMITH_API_KEY"):
        print("âš ï¸  LANGSMITH_API_KEY nÃ£o encontrada!")
        if not args.only_generate:
            print("   Evaluation nÃ£o serÃ¡ executada sem API key do LangSmith")
            print("   Use --only-generate para apenas gerar dados sintÃ©ticos")
        print()
    
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY nÃ£o encontrada!")
        print("   Configure sua API key OpenAI para continuar")
        sys.exit(1)
    
    # Executar pipeline
    pipeline = EvaluationPipeline(args)
    await pipeline.run_pipeline()


if __name__ == "__main__":
    asyncio.run(main()) 