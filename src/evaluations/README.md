# AvaliaÃ§Ã£o de Agentes Multi-Agent

## Fluxo de AvaliaÃ§Ã£o

```
---
config:
  layout: dagre
---
flowchart TD
 subgraph PIPELINE_EVAL_REFINEMENT["ğŸ§ª Pipeline de Evaluation &amp; Refinamento"]
        A1["ğŸ“¥ ExtraÃ§Ã£o de Traces"]
        A2["ğŸ§¼ Limpeza &amp; AnonimizaÃ§Ã£o"]
        A3["ğŸ“Š Dataset de AvaliaÃ§Ã£o"]
        B1["ğŸ§  ExecuÃ§Ã£o de MÃ©tricas perfil-specific"]
        B2["ğŸ§© Trajectory Fidelity"]
        B3["ğŸ›  Tool Usage Relevance"]
        B4["ğŸ§¾ Faithfulness / Relevance / Correctness"]
        C1["ğŸ“ˆ AtualizaÃ§Ã£o de Indicadores"]
        D1["ğŸ” SeleÃ§Ã£o automÃ¡tica de exemplos crÃ­ticos"]
        D2{"âš™ï¸ Verifica thresholds: seguir refinamento?"}
        D3A["ğŸ§¬ PreparaÃ§Ã£o/Filtro para Fine-Tuning"]
        D3B["ğŸ” InÃ­cio do Subflow de MetaPrompting"]
        F1["ğŸš€ ImplantaÃ§Ã£o"]
        F2["ğŸ§ª Teste de RegressÃ£o pÃ³s-deploy"]
        F3["ğŸ·ï¸ Registro &amp; PromoÃ§Ã£o"]
        P1["ğŸ“„ Prompt Base + Exemplos"]
  end
 subgraph MetaPrompting_Subflow["ğŸ” Subflow: MetaPrompting Iterativo"]
        P2["ğŸ§  Refinamento via LLM"]
        P3["ğŸ“Š ValidaÃ§Ã£o estrutural"]
        P4{"Refina novamente?"}
        P5["ğŸ” Novo Ciclo"]
        P6["âœ… Enviar para AvaliaÃ§Ã£o AutomÃ¡tica"]
  end
 subgraph INFRA_ORQUESTRACAO["ğŸ§© Infraestrutura de OrquestraÃ§Ã£o"]
        G1["âš™ï¸ Airflow DAG ou Prefect Pipeline"]
        G2["â±ï¸ Agendamento Recorrente"]
        G3["ğŸ“¦ Paralelismo por Agente/Job"]
        G4["ğŸ”„ Retry, Observabilidade, Logs"]
  end
    A1 --> A2
    A2 --> A3
    A3 --> B1
    B1 --> B2 & B3 & B4
    B2 --> C1
    B3 --> C1
    B4 --> C1
    C1 --> D1
    D1 --> D2
    D2 -- NÃ£o precisa --> F3
    D2 -- "Precisa fine-tune" --> D3A
    D3A --> F1
    D2 -- Refino via prompt --> D3B
    D3B --> P1
    F1 --> F2
    F2 --> F3
    P1 --> P2
    P2 --> P3
    P3 --> P4
    P4 -- Sim --> P5
    P5 --> P2
    P4 -- NÃ£o --> P6
    P6 L_P6_B1_0@--> B1
    G1 --> G2
    G2 --> G3
    G3 --> G4
    G1 -. executa ciclicamente .-> A1
    F3 -. Trigger externo (CI/CD ou Airflow scheduler) .-> G1
    L_P6_B1_0@{ animation: slow }

```
ğŸ§  DecisÃµes Arquiteturais

1. Seletividade com base em Thresholds

ApÃ³s o cÃ¡lculo das mÃ©tricas, o sistema verifica se os scores obtidos estÃ£o dentro dos limites definidos por perfil.

Caso afirmativo, evita custos desnecessÃ¡rios de refinamento.

2. ExecuÃ§Ã£o de MÃ©tricas conforme o Perfil

O pipeline identifica o perfil do dataset (agentic, llm_io, rag, tool_call) e aplica apenas as mÃ©tricas relevantes.

Isso permite flexibilidade e uso eficiente de recursos.

3. MetaPrompting Iterativo como Alternativa ao Fine-Tuning

Quando o problema nÃ£o exige re-treinamento do modelo, mas sim reformulaÃ§Ã£o do prompt, ativamos o subflow MetaPrompting.

Ele aplica LLM para reescrita de prompts usando templates Jinja2 + validaÃ§Ã£o estrutural (via AST/XML ou heurÃ­sticas).

4. Limpeza & AnonimizaÃ§Ã£o como PrÃ©-requisito

Antes de qualquer avaliaÃ§Ã£o, as entradas e saÃ­das passam por prÃ©-processamento com regex, heurÃ­sticas ou ferramentas como Microsoft Presidio.

5. OrquestraÃ§Ã£o Modular

A execuÃ§Ã£o ocorre via pipelines agendÃ¡veis (Airflow, Prefect), com suporte a paralelismo, retries e logs detalhados.

6. Ciclo Completo ObservÃ¡vel

Todos os resultados sÃ£o registrados via LangSmith ou LangFuse com tags, run_ids, thresholds e versÃ£o do prompt/model.

Este modelo representa a fundaÃ§Ã£o para avaliaÃ§Ãµes contÃ­nuas de agentes inteligentes e pipelines LLM com ciclo de feedback fechado e refinamento progressivo.

Essa arquitetura Ã© compatÃ­vel com ferramentas como:

DeepEval: mÃ©tricas semÃ¢nticas (faithfulness, relevance)

AgentEval: mÃ©tricas de fluxo e tool-usage

LangSmith / LangFuse: rastreamento, visualizaÃ§Ã£o e versionamento

## Ferramentas de AvaliaÃ§Ã£o

### OpenEvals
- Framework de avaliaÃ§Ã£o open-source para modelos de linguagem. Ã© seu ponto de partida leve e prÃ¡tico para testes automatizados e integra-se bem a CI/CD.

### DeepEval
Proporciona avaliaÃ§Ã£o de saÃ­das de LLMs em geral, cobrindo aspectos como:
- HallucinaÃ§Ãµes
- RelevÃ¢ncia
- Fidelidade
- ViÃ©s

**CaracterÃ­sticas:**
- IntegraÃ§Ã£o com pipelines de teste (usando Pytest)
- GeraÃ§Ã£o de datasets sintÃ©ticos
- Monitoramento em produÃ§Ã£o
- Ideal para benchmarking de respostas
- ComparaÃ§Ãµes de versÃµes de modelo
- AvaliaÃ§Ã£o nitidamente detalhada

**Recursos:**
- [GitHub](https://github.com/confident-ai/deepeval)
- [DEV Community](https://dev.to/confident-ai)
- [DocumentaÃ§Ã£o](https://docs.confident-ai.com/)

### AgentEval
Foi criado especificamente para avaliar agentes baseados em LLMs, ou seja, sistemas que realizam:
- Tarefas compostas
- Uso de ferramentas
- RaciocÃ­nio de mÃºltiplos passos
- Autonomia

**Arquitetura:**
- **CriticAgent**: Estrutura critÃ©rios de avaliaÃ§Ã£o
- **QuantifierAgent**: Quantifica mÃ©tricas
- **VerifierAgent**: Verifica resultados

**Foco:**
- Medir desempenho em tarefas especÃ­ficas
- EficiÃªncia
- Completude
- Uso correto de ferramentas

**Recursos:**
- [Medium](https://medium.com/@confident-ai)
- [Blog Incubyte](https://blog.incubyte.co/)
- [Confident AI](https://confident-ai.com/) (fonte original Microsoft)

## Quando Usar Cada Ferramenta

| SituaÃ§Ã£o | O que usar |
|----------|------------|
| **VocÃª quer medir respostas isoladas** (fidelidade, precisÃ£o, coerÃªncia) | Use **DeepEval** |
| **VocÃª quer medir o desempenho de um agente completo** (workflow, uso de ferramentas, planejamento) | Use **AgentEval** |
| **Quer testar qualidade de cada componente do agente e tambÃ©m resultados finais** | Combine ambos: ***DeepEval*** avalia as saÃ­das e comportamentos, enquanto ***AgentEval*** monitora o fluxo do agente como um todo |



# Arquitetura orientada a perfis (profile-specific)

| Tipo de Fluxo         | Exemplo                                 | CaracterÃ­sticas-chave                                         | MÃ©tricas relevantes                       |
| --------------------- | --------------------------------------- | ------------------------------------------------------------- | ----------------------------------------- |
| ğŸ” Agent-based        | LangGraph com mÃºltiplos nÃ³s             | HistÃ³rico com `intermediate_steps`, `tool_calls`, `state`     | `Trajectory Fidelity`, `Tool Usage`, etc. |
| ğŸ’¬ LLM pura (Chat)    | Input simples + Output                  | Apenas `input`, `output`                                      | `Faithfulness`, `Correctness`, `Toxicity` |
| ğŸ“š RAG                | Retrieval + Answer                      | Input com contexto + documentos recuperados + resposta gerada | `Context Recall`, `Answer Groundedness`   |
| ğŸ› ï¸ Tool-calling LLMs | Modelos que chamam ferramentas via JSON | ContÃªm `tool_calls` e `tool_results` explÃ­citos               | `Tool Usage Accuracy`, `Correctness`      |


## ETAPAS


| Etapa                     | Arquivo                                            |
| ------------------------- | -------------------------------------------------- |
| ğŸ“¥ ExtraÃ§Ã£o de traces     | `evaluations/ingestion/extract_traces.py`          |
| ğŸ§¼ Limpeza & AnonimizaÃ§Ã£o | `evaluations/preprocessing/clean_and_format.py`    |
| ğŸ“Š CriaÃ§Ã£o do Dataset     | `evaluations/datasets/build_evaluation_set.py`     |
| Dataset Builder (agentic) | `evaluations/datasets/builders/agentic_builder.py` |


### AvaliaÃ§Ã£o
| Etapa                                     | SugestÃ£o de Path                                | ObservaÃ§Ãµes                                                 |
| ----------------------------------------- | ----------------------------------------------- | ----------------------------------------------------------- |
| ğŸ§  ExecuÃ§Ã£o de MÃ©tricas perfil-specific   | `evaluations/evaluators/runner.py`              | Executor central que chama os avaliadores conforme o perfil |
| ğŸ§© Trajectory Fidelity                    | `evaluations/evaluators/trajectory_fidelity.py` | MÃ©trica especÃ­fica para agentes                             |
| ğŸ›  Tool Usage Relevance                   | `evaluations/evaluators/tool_usage.py`          | MÃ©trica baseada nas tool calls usadas                       |
| ğŸ§¾ Faithfulness / Relevance / Correctness | `evaluations/evaluators/semantic_metrics.py`    | Pode usar DeepEval ou LangSmith                             |


```bash
evaluations/
â”œâ”€â”€ evaluators/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluator_registry.py       âœ…
â”‚   â”œâ”€â”€ run_evaluations.py          âœ…
â”‚   â”œâ”€â”€ base.py                     âœ… Interface base
â”‚   â”œâ”€â”€ trajectory_fidelity.py      ğŸ”„ Agentic only
â”‚   â”œâ”€â”€ tool_usage_relevance.py     ğŸ”„ Agentic + RAG
â”‚   â”œâ”€â”€ faithfulness.py             ğŸ”„ Todos
â”‚   â”œâ”€â”€ correctness.py              ğŸ”„ Todos
â”‚   â”œâ”€â”€ relevance.py                ğŸ”„ Todos
â”‚   â””â”€â”€ hallucination_detection.py  ğŸ§ª Opcional avanÃ§ado
```

```bash
python evaluations/evaluators/run_evaluations.py \
  --dataset_name="fincoder_agentic_v1" \
  --dataset_profile="agentic" \
  --project_name="agent_eval_2025" \
  --tags="['evaluation','v1']"
```

#### MÃ©tricas & Avaliadores
| Arquivo                      | DescriÃ§Ã£o                                                               | Fluxos AplicÃ¡veis       | IntegraÃ§Ã£o no Diagrama            |
| ---------------------------- | ----------------------------------------------------------------------- | ----------------------- | --------------------------------- |
| `trajectory_fidelity.py`     | Avalia se os agentes seguiram a trajetÃ³ria esperada (nodes, transiÃ§Ãµes) | `agentic`               | `ğŸ§© Trajectory Fidelity`          |
| `tool_usage_relevance.py`    | Verifica se ferramentas chamadas eram relevantes para o objetivo        | `agentic`, `rag`        | `ğŸ›  Tool Usage Relevance`         |
| `faithfulness.py`            | Verifica se a resposta estÃ¡ alinhada com a fonte/contexto               | `rag`, `llm_io`, `chat` | `ğŸ§¾ Faithfulness`                 |
| `correctness.py`             | Compara resposta gerada com ground-truth para exatidÃ£o factual          | `llm_io`, `chat`        | `ğŸ§¾ Correctness`                  |
| `relevance.py`               | Verifica se a resposta Ã© relevante Ã  pergunta                           | `chat`, `llm_io`, `rag` | `ğŸ§¾ Relevance`                    |
| `hallucination_detection.py` | Detecta possÃ­veis alucinaÃ§Ãµes com base no contexto fornecido            | `rag`, `chat`           | `ğŸ§¾ Faithfulness / Hallucination` |


##### `trajectory_fidelity`
1. DeepEval (GitHub - langchain-ai/deepeval)
MÃ©trica anÃ¡loga: Step Consistency, Step Correctness ou Sequential Coherence.

Estado atual: o DeepEval nÃ£o tem uma mÃ©trica explÃ­cita chamada trajectory_fidelity, mas oferece ferramentas para testes por step, que podem ser adaptadas.

RecomendaÃ§Ã£o: se deseja seguir com DeepEval, vocÃª terÃ¡ que criar um CustomEvaluator usando as etapas (steps) dos agentes.

ğŸ”§ Exemplo (nÃ£o pronto, sÃ³ base):

```python
from deepeval.metrics.step_consistency import StepConsistencyMetric
metric = StepConsistencyMetric()
```


###  SeleÃ§Ã£o e DecisÃ£o

| Etapa                           | SugestÃ£o de Path                                    | ObservaÃ§Ãµes                             |
| ------------------------------- | --------------------------------------------------- | --------------------------------------- |
| ğŸ” SeleÃ§Ã£o de exemplos crÃ­ticos | `evaluations/selection/select_critical_examples.py` | LLM + heurÃ­stica                        |
| âš™ï¸ Verifica thresholds          | `evaluations/selection/evaluate_thresholds.py`      | Decide se segue para refinamento ou nÃ£o |

### Fine-Tuning

| Etapa                          | SugestÃ£o de Path                       | ObservaÃ§Ãµes                                  |
| ------------------------------ | -------------------------------------- | -------------------------------------------- |
| PreparaÃ§Ã£o/Filtro Fine-Tuning  | `evaluations/finetune/prepare_data.py` | Extrai exemplos relevantes e formata para FT |
| Envio para FT (ex: OpenAI API) | `evaluations/finetune/run_finetune.py` | Possivelmente externo                        |

###  MetaPrompting (Subflow)

| Etapa                                  | SugestÃ£o de Path                                         | ObservaÃ§Ãµes                    |
| -------------------------------------- | -------------------------------------------------------- | ------------------------------ |
| Template Base + Exemplos               | `evaluations/prompt_tuning/templates/base_prompt.jinja2` | Prompt principal com slots     |
| Refinamento via LLM                    | `evaluations/prompt_tuning/metaprompt_llm.py`            | Chamada LLM para reescrita     |
| ValidaÃ§Ã£o estrutural do novo prompt    | `evaluations/prompt_tuning/validate_prompt.py`           | Pode usar AST/XML ou LangSmith |
| AvaliaÃ§Ã£o automÃ¡tica dos novos prompts | Reaproveita `evaluators/runner.py`                       | Retesta os prompts gerados     |


### Deploy & Versionamento

| Etapa                         | SugestÃ£o de Path                             | ObservaÃ§Ãµes                      |
| ----------------------------- | -------------------------------------------- | -------------------------------- |
| ImplantaÃ§Ã£o de prompts        | `evaluations/prompt_tuning/deploy_prompt.py` | Usa LangSmith Hub                |
| Teste de RegressÃ£o pÃ³s-deploy | `evaluations/tests/post_deploy_test.py`      | Executa dataset antigo e compara |
| Registro & PromoÃ§Ã£o           | `evaluations/registry/promote_version.py`    | Atualiza versÃ£o oficial          |

