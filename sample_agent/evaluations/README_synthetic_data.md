# Gerador de Dados Sint√©ticos para Sistema de Swarm Agents

Este sistema gera dados sint√©ticos para avaliar o desempenho dos agentes no seu sistema de swarm, integrando com LangSmith para coleta autom√°tica de traces e executando evaluators para an√°lise de performance.

## üéØ Vis√£o Geral

O sistema √© composto por tr√™s componentes principais:

1. **Gerador de Dados Sint√©ticos**: Cria cen√°rios realistas baseados no comportamento dos agentes
2. **Executor de Cen√°rios**: Executa os cen√°rios nos agentes reais para coletar traces
3. **Pipeline de Evaluation**: Integra gera√ß√£o de dados com suite de evaluators

## üèóÔ∏è Arquitetura

### Agentes Suportados

- **Alice Agent**: Especialista em matem√°tica com tool `calculate_math`
- **Bob Agent**: Especialista em clima (fala como pirata) com tools `get_weather` e `ask_user`
- **Main Agent**: Coordenador que gerencia tarefas e handoffs entre agentes

### Tipos de Cen√°rios

1. **Simples**: Opera√ß√µes b√°sicas (2+2, clima de SP)
2. **M√©dios**: Opera√ß√µes intermedi√°rias (express√µes compostas, cidades espec√≠ficas)
3. **Complexos**: Opera√ß√µes avan√ßadas (express√µes aninhadas, coordena√ß√£o multi-agente)
4. **Edge Cases**: Casos extremos para testar robustez

## üöÄ Uso R√°pido

### Configura√ß√£o Inicial

```bash
# Configurar vari√°veis de ambiente
export OPENAI_API_KEY="sua_api_key_aqui"
export LANGSMITH_API_KEY="sua_langsmith_api_key_aqui"
export LANGCHAIN_TRACING_V2="true"

# Instalar depend√™ncias
pip install -r requirements.txt
```

### Teste R√°pido

```bash
# Executar pipeline completo (3 cen√°rios por agente)
python sample_agent/evaluations/run_evaluation_pipeline.py --quick-test

# Apenas gerar dados sint√©ticos
python sample_agent/evaluations/run_synthetic_data_generator.py --quick-test
```

### Execu√ß√£o Completa

```bash
# Pipeline completo com configura√ß√£o personalizada
python sample_agent/evaluations/run_evaluation_pipeline.py \
  --project "minha-avaliacao" \
  --scenarios 20 \
  --model "openai:gpt-4o" \
  --evaluator-profile "comprehensive"
```

## üìö Scripts Dispon√≠veis

### 1. `synthetic_data_generator.py`

**Funcionalidade**: N√∫cleo do sistema de gera√ß√£o de dados sint√©ticos.

**Caracter√≠sticas**:
- Gera cen√°rios baseados no comportamento dos agentes
- Executa cen√°rios nos agentes reais
- Coleta traces automaticamente via LangSmith
- Gera relat√≥rios detalhados

**Uso program√°tico**:
```python
from sample_agent.evaluations.synthetic_data_generator import SyntheticDataGenerator

generator = SyntheticDataGenerator(
    model_name="openai:gpt-4o-mini",
    temperature=0.7,
    langsmith_project_name="meu-projeto",
    num_scenarios_per_agent=15
)

scenarios = generator.generate_all_scenarios()
results = await generator.execute_all_scenarios(scenarios)
report = generator.generate_report(results)
```

### 2. `run_synthetic_data_generator.py`

**Funcionalidade**: Interface de linha de comando para gera√ß√£o de dados sint√©ticos.

**Par√¢metros principais**:
- `--project`: Nome do projeto LangSmith
- `--scenarios`: N√∫mero de cen√°rios por agente (padr√£o: 15)
- `--model`: Modelo a usar (padr√£o: openai:gpt-4o-mini)
- `--temperature`: Temperatura do modelo (padr√£o: 0.7)
- `--quick-test`: Modo teste r√°pido (3 cen√°rios)

**Exemplos**:
```bash
# Execu√ß√£o b√°sica
python run_synthetic_data_generator.py

# Configura√ß√£o personalizada
python run_synthetic_data_generator.py \
  --project "avaliacao-agentes" \
  --scenarios 25 \
  --model "openai:gpt-4o" \
  --temperature 0.5

# Teste r√°pido
python run_synthetic_data_generator.py --quick-test
```

### 3. `run_evaluation_pipeline.py`

**Funcionalidade**: Pipeline completo que integra gera√ß√£o de dados com evaluation.

**Fases do Pipeline**:
1. **Gera√ß√£o de Dados**: Cria e executa cen√°rios sint√©ticos
2. **Evaluation**: Executa suite de evaluators nos traces coletados
3. **Relat√≥rio**: Gera relat√≥rio consolidado com m√©tricas

**Par√¢metros principais**:
- `--only-generate`: Apenas gerar dados sint√©ticos
- `--only-evaluate`: Apenas executar evaluators
- `--evaluator-profile`: Perfil de evaluators (minimal, agentic, comprehensive)
- `--evaluation-model`: Modelo para evaluation (padr√£o: gpt-4o)

**Exemplos**:
```bash
# Pipeline completo
python run_evaluation_pipeline.py \
  --project "pipeline-completo" \
  --scenarios 20 \
  --evaluator-profile "comprehensive"

# Apenas gera√ß√£o de dados
python run_evaluation_pipeline.py \
  --only-generate \
  --scenarios 15

# Apenas evaluation (dados existentes)
python run_evaluation_pipeline.py \
  --only-evaluate \
  --project "projeto-existente"
```

## üìä Outputs e Relat√≥rios

### Estrutura de Arquivos

```
sample_agent/evaluations/
‚îú‚îÄ‚îÄ results/                     # Diret√≥rio de resultados
‚îÇ   ‚îú‚îÄ‚îÄ synthetic_data_report_*.json
‚îÇ   ‚îú‚îÄ‚îÄ evaluation_results_*.json
‚îÇ   ‚îî‚îÄ‚îÄ consolidated_report_*.json
‚îú‚îÄ‚îÄ synthetic_data_generator.py
‚îú‚îÄ‚îÄ run_synthetic_data_generator.py
‚îî‚îÄ‚îÄ run_evaluation_pipeline.py
```

### Relat√≥rios Gerados

1. **Relat√≥rio de Dados Sint√©ticos** (`synthetic_data_report_*.json`):
   - Estat√≠sticas de gera√ß√£o e execu√ß√£o
   - M√©tricas por agente e complexidade
   - Cen√°rios que falharam
   - Tempos de execu√ß√£o

2. **Relat√≥rio de Evaluation** (`evaluation_results_*.json`):
   - Resultados dos evaluators
   - M√©tricas de performance
   - Scores detalhados por run

3. **Relat√≥rio Consolidado** (`consolidated_report_*.json`):
   - Vis√£o geral do pipeline
   - M√©tricas combinadas
   - Resumo executivo

### Exemplo de M√©tricas

```json
{
  "pipeline_summary": {
    "project_name": "minha-avaliacao",
    "overall_success_rate": 87.5,
    "total_avg_time": 2.3
  },
  "data_generation": {
    "total_scenarios": 45,
    "successful": 42,
    "success_rate": 93.3
  },
  "evaluation_results": {
    "total_runs": 42,
    "evaluators_executed": 6,
    "avg_scores": {
      "faithfulness": 0.85,
      "relevance": 0.89,
      "tool_usage": 0.92
    }
  }
}
```

## üîß Configura√ß√£o Avan√ßada

### Personaliza√ß√£o de Cen√°rios

Para adicionar novos tipos de cen√°rios, edite os templates em `synthetic_data_generator.py`:

```python
# Adicionar novos cen√°rios para Alice
self.alice_scenarios.append({
    "type": "custom",
    "template": "Resolva o problema: {expression}",
    "expressions": ["nova_expressao_1", "nova_expressao_2"]
})
```

### Configura√ß√£o de Evaluators

O sistema usa perfis de evaluators configur√°veis:

- **minimal**: Apenas evaluators b√°sicos
- **agentic**: Evaluators espec√≠ficos para agentes
- **comprehensive**: Todos os evaluators dispon√≠veis

### Integra√ß√£o com LangSmith

O sistema configura automaticamente:
- Nome do projeto programaticamente
- Coleta de traces habilitada
- Metadados de execu√ß√£o

## üéØ Casos de Uso

### 1. Desenvolvimento de Agentes

```bash
# Teste r√°pido durante desenvolvimento
python run_evaluation_pipeline.py --quick-test

# Evaluation completa antes de deploy
python run_evaluation_pipeline.py \
  --project "pre-deploy-validation" \
  --scenarios 30 \
  --evaluator-profile "comprehensive"
```

### 2. An√°lise de Performance

```bash
# Comparar diferentes modelos
python run_evaluation_pipeline.py \
  --project "gpt-4o-vs-4o-mini" \
  --model "openai:gpt-4o" \
  --scenarios 25

python run_evaluation_pipeline.py \
  --project "gpt-4o-vs-4o-mini" \
  --model "openai:gpt-4o-mini" \
  --scenarios 25
```

### 3. Regression Testing

```bash
# Gerar baseline
python run_evaluation_pipeline.py \
  --project "baseline-v1" \
  --scenarios 50

# Comparar ap√≥s mudan√ßas
python run_evaluation_pipeline.py \
  --project "after-changes-v1" \
  --scenarios 50
```

## üîç Troubleshooting

### Problemas Comuns

1. **Erro de API Key**:
   ```
   ‚ö†Ô∏è  OPENAI_API_KEY n√£o encontrada!
   ```
   **Solu√ß√£o**: Configure `export OPENAI_API_KEY="sua_key"`

2. **LangSmith n√£o coleta traces**:
   ```
   ‚ö†Ô∏è  LANGSMITH_API_KEY n√£o encontrada!
   ```
   **Solu√ß√£o**: Configure `export LANGSMITH_API_KEY="sua_key"`

3. **Agents n√£o respondem**:
   - Verifique se os agentes est√£o configurados corretamente
   - Teste com `--quick-test` primeiro
   - Verifique logs de erro com `--verbose`

### Debugging

```bash
# Execu√ß√£o com logs detalhados
python run_evaluation_pipeline.py --verbose --quick-test

# Apenas gera√ß√£o para debug
python run_synthetic_data_generator.py --scenarios 3 --verbose
```

## üìà Pr√≥ximos Passos

1. **An√°lise de Resultados**: Examine os relat√≥rios JSON gerados
2. **LangSmith Dashboard**: Visualize traces e m√©tricas
3. **Otimiza√ß√£o**: Ajuste configura√ß√µes baseado nos resultados
4. **Itera√ß√£o**: Execute novamente com configura√ß√µes otimizadas

## ü§ù Contribui√ß√£o

Para adicionar novos tipos de cen√°rios ou evaluators:

1. Edite os templates em `synthetic_data_generator.py`
2. Adicione novos evaluators em `evaluators/`
3. Registre no `evaluator_registry.py`
4. Teste com `--quick-test`

---

**Pronto para usar!** Execute `python run_evaluation_pipeline.py --quick-test` para come√ßar. 